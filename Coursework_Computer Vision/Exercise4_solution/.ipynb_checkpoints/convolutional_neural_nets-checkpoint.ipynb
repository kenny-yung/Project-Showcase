{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d9dbc74c08a75dc1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Important: Please run the full notebook before submitting!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-67e1fcdd86afffc8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Convolutional Neural Networks\n",
    "\n",
    "In this exercise you will be introduced to some practical aspects of deep learning in\n",
    "computer vision, including constructing a deep neural network and training it via gradient\n",
    "descent to tackle image classification.\n",
    "\n",
    "We will use the popular TensorFlow framework through the Keras API.\n",
    "\n",
    "We will tackle **image classification** through deep learning methods, in particular we will look at\n",
    "\n",
    "* Dataset download and normalization\n",
    "* Softmax regression with stochastic gradient descent and Adam\n",
    "* Multilayer perceptrons with tanh and ReLU\n",
    "* A basic convolutional net\n",
    "* BatchNorm, striding, global average pooling\n",
    "* Residual networks\n",
    "* Learning rate decay and data augmentation\n",
    "\n",
    "\n",
    "### Install TensorFlow\n",
    "\n",
    "Install TensorFlow using `pip install tensorflow`. TensorFlow can use GPUs to make the training several times faster, but since not all of you may have access to a GPU, we have tried to scale this exercise with a CPU in mind. For more info on installing TensorFlow, see https://www.tensorflow.org/install/pip.\n",
    "\n",
    "### TensorBoard Plotting\n",
    "\n",
    "TensorBoard is a web-based tool for drawing pretty plots of quantities we care about during training, such as the loss. We need to choose a folder where these values will be stored (\"logdir\").\n",
    "\n",
    "Start the TensorBoard server by executing e.g. `tensorboard --logdir /tmp/tensorboard_logs` after you've activated your conda environment. If you change the logdir, also adjust it in the cell below.\n",
    "\n",
    "You can view the graphs by visiting http://localhost:6006 in your browser (6006 is the default port).\n",
    "At first there will be nothing to plot, so it will be empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_root = 'tensorboard_logs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-61c4af0db2f8c0b1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<!-- Run this cell to add heading letters per subtask (like a, b, c) -->\n",
       "<style>\n",
       "body {counter-reset: section;}\n",
       "h2:before {counter-increment: section;\n",
       "           content: counter(section, lower-alpha) \") \";}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<!-- Run this cell to add heading letters per subtask (like a, b, c) -->\n",
    "<style>\n",
    "body {counter-reset: section;}\n",
    "h2:before {counter-increment: section;\n",
    "           content: counter(section, lower-alpha) \") \";}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1073cd4f1b08084c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import datetime\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio\n",
    "import cv2\n",
    "import tensorflow.keras.models as models\n",
    "import tensorflow.keras.layers as layers\n",
    "import tensorflow.keras.regularizers as regularizers\n",
    "import tensorflow.keras.optimizers as optimizers\n",
    "import tensorflow.keras.callbacks as callbacks\n",
    "import tensorflow.keras.initializers as initializers\n",
    "import tensorflow.keras.preprocessing.image as kerasimage\n",
    "\n",
    "# Just an image plotting function\n",
    "def plot_multiple(images, titles=None, colormap='gray',\n",
    "                  max_columns=np.inf, imwidth=4, imheight=4, share_axes=False):\n",
    "    \"\"\"Plot multiple images as subplots on a grid.\"\"\"\n",
    "    if titles is None:\n",
    "        titles = [''] *len(images)\n",
    "    assert len(images) == len(titles)\n",
    "    n_images = len(images)\n",
    "    n_cols = min(max_columns, n_images)\n",
    "    n_rows = int(np.ceil(n_images / n_cols))\n",
    "    fig, axes = plt.subplots(\n",
    "        n_rows, n_cols, figsize=(n_cols * imwidth, n_rows * imheight),\n",
    "        squeeze=False, sharex=share_axes, sharey=share_axes)\n",
    "\n",
    "    axes = axes.flat\n",
    "    # Hide subplots without content\n",
    "    for ax in axes[n_images:]:\n",
    "        ax.axis('off')\n",
    "        \n",
    "    if not isinstance(colormap, (list,tuple)):\n",
    "        colormaps = [colormap]*n_images\n",
    "    else:\n",
    "        colormaps = colormap\n",
    "\n",
    "    for ax, image, title, cmap in zip(axes, images, titles, colormaps):\n",
    "        ax.imshow(image, cmap=cmap)\n",
    "        ax.set_title(title)\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "        \n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-258c52e8e84d0db9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Dataset Preparation\n",
    "We are going to tackle the classic image classification task using the **CIFAR-10 dataset**, containing 60,000 32x32 RGB images of 10 different classes (50,000 for training and 10,000 for testing). \n",
    "\n",
    "![image.png](cifar.png)\n",
    "\n",
    "The dataset is automatically downloaded if you run the next cell.\n",
    "You may read more about the dataset at https://www.cs.toronto.edu/~kriz/cifar.html.\n",
    "\n",
    "A common normalization strategy is to map the image RGB values to the range 0-1 and to subtract the mean training pixel value. Perform this normalization below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-ea360dbfd9ffa26b",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170500096/170498071 [==============================] - 7s 0us/step\n"
     ]
    }
   ],
   "source": [
    "(im_train, y_train), (im_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "# Normalize to 0-1 range and subtract mean of training pixels\n",
    "### BEGIN SOLUTION\n",
    "im_train = im_train / 255\n",
    "im_test = im_test / 255\n",
    "\n",
    "mean_training_pixel = np.mean(im_train, axis=(0,1,2))\n",
    "x_train = im_train - mean_training_pixel\n",
    "x_test = im_test - mean_training_pixel\n",
    "### END SOLUTION\n",
    "\n",
    "image_shape = x_train[0].shape\n",
    "labels = ['airplane','automobile','bird','cat','deer','dog','frog','horse','ship','truck']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e030eef482140b24",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Softmax Regression\n",
    "\n",
    "Before considering convolutional neural networks, let us start with a simpler classifier called softmax regression (a.k.a. multinomial logistic regression). Note that even though the name contains \"regression\", this is a classification model.\n",
    "\n",
    "Softmax regression can be understood as a single-layer neural network. We first flatten our input image to a long vector $\\mathbf{x}$, consisting of $32\\cdot 32\\cdot 3= 3072$ values. Then we predict class probabilities $\\hat{\\mathbf{y}}$ through a fully-connected layer with softmax activation:\n",
    "\n",
    "$$\n",
    "\\mathbf{z} = W \\mathbf{x} + \\mathbf{b} \\\\\n",
    "\\hat{y}_c = \\operatorname{softmax}(\\mathbf{z})_c = \\frac{\\exp{z_c}}{\\sum_{\\tilde{c}=1}^{10} \\exp{z_{\\tilde{c}}}}\n",
    "$$\n",
    "\n",
    "Here $z_c$ denotes the $c$th component of the vector $\\mathbf{z}$, called the vector of **logits**.\n",
    "The weights $W$ and biases $\\mathbf{b}$ will be learned during training.\n",
    "\n",
    "### Training\n",
    "\n",
    "We train the model by minimizing a **loss function** averaged over the training data. As we are tackling a classification problem, the **cross-entropy** is a suitable loss function:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{CE}(\\mathbf{y}, \\hat{\\mathbf{y}}; W, \\mathbf{b}) = - \\sum_{c=1}^{10} y_c \\log{\\hat{y}_c}\n",
    "$$\n",
    "\n",
    "Note that in the above notation the ground-truth $\\mathbf{y}$ is a so-called **one-hot vector**, containing a single 1 component, while the remaining components \n",
    "are zeros. The model's predicted $\\hat{\\mathbf{y}}$ is a vector which also sums to one, but whose components all take continuous values in the range $(0, 1)$.\n",
    "\n",
    "We minimize the loss by **stochastic gradient descent** (SGD). That is, we repeatedly sample mini-batches from the training data and update the parameters (weights and biases) towards the direction of the steepest decrease of the loss averaged over the mini-batch. For example, the weight $w_{ij}$ (an element of the matrix $W$) is updated according to:\n",
    "\n",
    "$$\n",
    "w_{ij}^{(t+1)} = w_{ij}^{(t)} - \\eta \\cdot \\frac{\\partial \\mathcal{L}_{CE}} {\\partial w_{ij}},\n",
    "$$\n",
    "\n",
    "with $\\eta$ being the learning rate.\n",
    "\n",
    "----\n",
    "\n",
    "This is all very straightforward to perform in Keras. `models.Sequential` accepts a list of layers that are applied sequentially, in a chain. Here we have two layers, `Flatten` to convert the image into a long vector and `Dense`, which is a synonym for fully-connected layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5c34f08b3b61750a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/70\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 1.9976 - accuracy: 0.2976 - val_loss: 1.8952 - val_accuracy: 0.3523\n",
      "Epoch 2/70\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 1.8666 - accuracy: 0.3611 - val_loss: 1.8366 - val_accuracy: 0.3717\n",
      "Epoch 3/70\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 1.8241 - accuracy: 0.3773 - val_loss: 1.8091 - val_accuracy: 0.3775\n",
      "Epoch 4/70\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 1.7990 - accuracy: 0.3872 - val_loss: 1.7889 - val_accuracy: 0.3834\n",
      "Epoch 5/70\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 1.7813 - accuracy: 0.3926 - val_loss: 1.7755 - val_accuracy: 0.3913\n",
      "Epoch 6/70\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 1.7681 - accuracy: 0.3974 - val_loss: 1.7648 - val_accuracy: 0.3954\n",
      "Epoch 7/70\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 1.7575 - accuracy: 0.4015 - val_loss: 1.7570 - val_accuracy: 0.3960\n",
      "Epoch 8/70\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 1.7488 - accuracy: 0.4051 - val_loss: 1.7507 - val_accuracy: 0.3992\n",
      "Epoch 9/70\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 1.7415 - accuracy: 0.4073 - val_loss: 1.7452 - val_accuracy: 0.4008\n",
      "Epoch 10/70\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 1.7353 - accuracy: 0.4086 - val_loss: 1.7402 - val_accuracy: 0.4031\n",
      "Epoch 11/70\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 1.7295 - accuracy: 0.4103 - val_loss: 1.7375 - val_accuracy: 0.4049\n",
      "Epoch 12/70\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 1.7246 - accuracy: 0.4120 - val_loss: 1.7339 - val_accuracy: 0.4026\n",
      "Epoch 13/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 1.7204 - accuracy: 0.4137 - val_loss: 1.7312 - val_accuracy: 0.4048\n",
      "Epoch 14/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 1.7162 - accuracy: 0.4147 - val_loss: 1.7267 - val_accuracy: 0.4062\n",
      "Epoch 15/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 1.7126 - accuracy: 0.4166 - val_loss: 1.7259 - val_accuracy: 0.4073\n",
      "Epoch 16/70\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 1.7095 - accuracy: 0.4174 - val_loss: 1.7233 - val_accuracy: 0.4054\n",
      "Epoch 17/70\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 1.7065 - accuracy: 0.4185 - val_loss: 1.7211 - val_accuracy: 0.4061\n",
      "Epoch 18/70\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 1.7037 - accuracy: 0.4198 - val_loss: 1.7199 - val_accuracy: 0.4059\n",
      "Epoch 19/70\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 1.7007 - accuracy: 0.4199 - val_loss: 1.7185 - val_accuracy: 0.4071\n",
      "Epoch 20/70\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 1.6986 - accuracy: 0.4219 - val_loss: 1.7175 - val_accuracy: 0.4114\n",
      "Epoch 21/70\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 1.6964 - accuracy: 0.4236 - val_loss: 1.7153 - val_accuracy: 0.4066\n",
      "Epoch 22/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 1.6940 - accuracy: 0.4240 - val_loss: 1.7147 - val_accuracy: 0.4086\n",
      "Epoch 23/70\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 1.6923 - accuracy: 0.4245 - val_loss: 1.7140 - val_accuracy: 0.4095\n",
      "Epoch 24/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 1.6903 - accuracy: 0.4254 - val_loss: 1.7127 - val_accuracy: 0.4085\n",
      "Epoch 25/70\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 1.6886 - accuracy: 0.4253 - val_loss: 1.7121 - val_accuracy: 0.4104\n",
      "Epoch 26/70\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 1.6871 - accuracy: 0.4266 - val_loss: 1.7118 - val_accuracy: 0.4099\n",
      "Epoch 27/70\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 1.6853 - accuracy: 0.4270 - val_loss: 1.7105 - val_accuracy: 0.4093\n",
      "Epoch 28/70\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 1.6836 - accuracy: 0.4271 - val_loss: 1.7102 - val_accuracy: 0.4100\n",
      "Epoch 29/70\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 1.6822 - accuracy: 0.4269 - val_loss: 1.7107 - val_accuracy: 0.4130\n",
      "Epoch 30/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 1.6809 - accuracy: 0.4298 - val_loss: 1.7091 - val_accuracy: 0.4123\n",
      "Epoch 31/70\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 1.6795 - accuracy: 0.4292 - val_loss: 1.7080 - val_accuracy: 0.4121\n",
      "Epoch 32/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 1.6781 - accuracy: 0.4296 - val_loss: 1.7086 - val_accuracy: 0.4106\n",
      "Epoch 33/70\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 1.6771 - accuracy: 0.4303 - val_loss: 1.7075 - val_accuracy: 0.4117\n",
      "Epoch 34/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 1.6759 - accuracy: 0.4315 - val_loss: 1.7074 - val_accuracy: 0.4101\n",
      "Epoch 35/70\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 1.6745 - accuracy: 0.4319 - val_loss: 1.7063 - val_accuracy: 0.4126\n",
      "Epoch 36/70\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 1.6735 - accuracy: 0.4318 - val_loss: 1.7059 - val_accuracy: 0.4116\n",
      "Epoch 37/70\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 1.6722 - accuracy: 0.4331 - val_loss: 1.7066 - val_accuracy: 0.4122\n",
      "Epoch 38/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 1.6715 - accuracy: 0.4330 - val_loss: 1.7045 - val_accuracy: 0.4116\n",
      "Epoch 39/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 1.6704 - accuracy: 0.4330 - val_loss: 1.7056 - val_accuracy: 0.4124\n",
      "Epoch 40/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 1.6693 - accuracy: 0.4332 - val_loss: 1.7064 - val_accuracy: 0.4126\n",
      "Epoch 41/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 1.6684 - accuracy: 0.4338 - val_loss: 1.7040 - val_accuracy: 0.4140\n",
      "Epoch 42/70\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 1.6674 - accuracy: 0.4345 - val_loss: 1.7055 - val_accuracy: 0.4123\n",
      "Epoch 43/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 1.6666 - accuracy: 0.4354 - val_loss: 1.7041 - val_accuracy: 0.4117\n",
      "Epoch 44/70\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 1.6658 - accuracy: 0.4347 - val_loss: 1.7044 - val_accuracy: 0.4128\n",
      "Epoch 45/70\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 1.6647 - accuracy: 0.4353 - val_loss: 1.7043 - val_accuracy: 0.4120\n",
      "Epoch 46/70\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 1.6641 - accuracy: 0.4371 - val_loss: 1.7038 - val_accuracy: 0.4133\n",
      "Epoch 47/70\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 1.6631 - accuracy: 0.4368 - val_loss: 1.7040 - val_accuracy: 0.4139\n",
      "Epoch 48/70\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 1.6625 - accuracy: 0.4363 - val_loss: 1.7027 - val_accuracy: 0.4150\n",
      "Epoch 49/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 1.6615 - accuracy: 0.4369 - val_loss: 1.7032 - val_accuracy: 0.4116\n",
      "Epoch 50/70\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 1.6610 - accuracy: 0.4367 - val_loss: 1.7040 - val_accuracy: 0.4125\n",
      "Epoch 51/70\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 1.6601 - accuracy: 0.4370 - val_loss: 1.7040 - val_accuracy: 0.4140\n",
      "Epoch 52/70\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 1.6595 - accuracy: 0.4381 - val_loss: 1.7031 - val_accuracy: 0.4131\n",
      "Epoch 53/70\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 1.6587 - accuracy: 0.4378 - val_loss: 1.7030 - val_accuracy: 0.4142\n",
      "Epoch 54/70\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 1.6581 - accuracy: 0.4379 - val_loss: 1.7030 - val_accuracy: 0.4129\n",
      "Epoch 55/70\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 1.6575 - accuracy: 0.4391 - val_loss: 1.7031 - val_accuracy: 0.4124\n",
      "Epoch 56/70\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 1.6566 - accuracy: 0.4387 - val_loss: 1.7027 - val_accuracy: 0.4145\n",
      "Epoch 57/70\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 1.6561 - accuracy: 0.4396 - val_loss: 1.7046 - val_accuracy: 0.4118\n",
      "Epoch 58/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 2s 4ms/step - loss: 1.6556 - accuracy: 0.4380 - val_loss: 1.7057 - val_accuracy: 0.4120\n",
      "Epoch 59/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 1.6548 - accuracy: 0.4400 - val_loss: 1.7014 - val_accuracy: 0.4140\n",
      "Epoch 60/70\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 1.6541 - accuracy: 0.4409 - val_loss: 1.7029 - val_accuracy: 0.4135\n",
      "Epoch 61/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 1.6537 - accuracy: 0.4400 - val_loss: 1.7023 - val_accuracy: 0.4141\n",
      "Epoch 62/70\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 1.6532 - accuracy: 0.4407 - val_loss: 1.7026 - val_accuracy: 0.4147\n",
      "Epoch 63/70\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 1.6526 - accuracy: 0.4404 - val_loss: 1.7040 - val_accuracy: 0.4139\n",
      "Epoch 64/70\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 1.6519 - accuracy: 0.4405 - val_loss: 1.7030 - val_accuracy: 0.4152\n",
      "Epoch 65/70\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 1.6514 - accuracy: 0.4410 - val_loss: 1.7041 - val_accuracy: 0.4139\n",
      "Epoch 66/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 1.6508 - accuracy: 0.4420 - val_loss: 1.7024 - val_accuracy: 0.4141\n",
      "Epoch 67/70\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 1.6502 - accuracy: 0.4416 - val_loss: 1.7033 - val_accuracy: 0.4126\n",
      "Epoch 68/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 1.6500 - accuracy: 0.4421 - val_loss: 1.7021 - val_accuracy: 0.4153\n",
      "Epoch 69/70\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 1.6493 - accuracy: 0.4422 - val_loss: 1.7032 - val_accuracy: 0.4134\n",
      "Epoch 70/70\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 1.6488 - accuracy: 0.4419 - val_loss: 1.7020 - val_accuracy: 0.4147\n"
     ]
    }
   ],
   "source": [
    "softmax_regression = models.Sequential([\n",
    "    layers.Flatten(input_shape=image_shape),\n",
    "    layers.Dense(10, activation='softmax')],\n",
    "    name='linear')\n",
    "\n",
    "def train_model(model, batch_size=128, n_epochs=70,  optimizer=optimizers.SGD, learning_rate=1e-2):\n",
    "    opt = optimizer(lr=learning_rate)\n",
    "    model.compile(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    timestamp = datetime.datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "    logdir = os.path.join(log_root, f'{model.name}_{timestamp}')\n",
    "    tensorboard_callback = callbacks.TensorBoard(logdir)\n",
    "    model.fit(x=x_train, y=y_train, verbose=1, epochs=n_epochs, \n",
    "              validation_data=(x_test, y_test), batch_size=batch_size,\n",
    "              callbacks=[tensorboard_callback])\n",
    "\n",
    "train_model(softmax_regression, optimizer=optimizers.SGD, learning_rate=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6cc309c333f4c13a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "(Jupyter Notebook Tip: After you're done training, you can collapse or hide the output by clicking or double clicking the area directly to the left of the output.)\n",
    "\n",
    "You can check the how the loss and accuracy (= proportion of correctly predicted classes) change over the course of training in TensorBoard. What do you observe?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-268ce60b8704b0ab",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "Training curves get better during training, curves on evaluation split plateau after 40 epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ec4a31aa3b860db5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Adam Optimizer\n",
    "\n",
    "There has been a lot of research on improving on the simple stochastic gradient descent algorithm we used above. One of the most popular variants is called **Adam** (https://arxiv.org/abs/1412.6980, \"adaptive moment estimation\"). Its learning rate usually requires less precise tuning, and something in the range of $(10^{-4},10^{-3})$ often works well in practice. Intuitively, this is because the algorithm automatically adapts the learning rate for each weight depending on the gradients.\n",
    "\n",
    "You can run it as follows (the optimizer is passed to Keras's `model.fit` function in `train_model`). The difference is not large for such a simple model, but makes a bigger difference for larger networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-645a91d07f56cfbc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/70\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.9181 - accuracy: 0.3356 - val_loss: 1.8269 - val_accuracy: 0.3697\n",
      "Epoch 2/70\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 1.8032 - accuracy: 0.3864 - val_loss: 1.7854 - val_accuracy: 0.3872\n",
      "Epoch 3/70\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 1.7701 - accuracy: 0.3988 - val_loss: 1.7663 - val_accuracy: 0.3890\n",
      "Epoch 4/70\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 1.7489 - accuracy: 0.4046 - val_loss: 1.7499 - val_accuracy: 0.4030\n",
      "Epoch 5/70\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 1.7338 - accuracy: 0.4106 - val_loss: 1.7415 - val_accuracy: 0.4009\n",
      "Epoch 6/70\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 1.7221 - accuracy: 0.4133 - val_loss: 1.7328 - val_accuracy: 0.4050\n",
      "Epoch 7/70\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 1.7126 - accuracy: 0.4188 - val_loss: 1.7331 - val_accuracy: 0.4043\n",
      "Epoch 8/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 1.7055 - accuracy: 0.4203 - val_loss: 1.7231 - val_accuracy: 0.4105\n",
      "Epoch 9/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 1.6979 - accuracy: 0.4235 - val_loss: 1.7214 - val_accuracy: 0.4083\n",
      "Epoch 10/70\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 1.6932 - accuracy: 0.4247 - val_loss: 1.7185 - val_accuracy: 0.4084\n",
      "Epoch 11/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 1.6880 - accuracy: 0.4266 - val_loss: 1.7171 - val_accuracy: 0.4101\n",
      "Epoch 12/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 1.6835 - accuracy: 0.4286 - val_loss: 1.7137 - val_accuracy: 0.4122\n",
      "Epoch 13/70\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 1.6794 - accuracy: 0.4318 - val_loss: 1.7107 - val_accuracy: 0.4125\n",
      "Epoch 14/70\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 1.6757 - accuracy: 0.4308 - val_loss: 1.7124 - val_accuracy: 0.4075\n",
      "Epoch 15/70\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 1.6730 - accuracy: 0.4314 - val_loss: 1.7090 - val_accuracy: 0.4113\n",
      "Epoch 16/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 1.6701 - accuracy: 0.4334 - val_loss: 1.7104 - val_accuracy: 0.4085\n",
      "Epoch 17/70\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 1.6671 - accuracy: 0.4348 - val_loss: 1.7089 - val_accuracy: 0.4125\n",
      "Epoch 18/70\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.6647 - accuracy: 0.4358 - val_loss: 1.7102 - val_accuracy: 0.4119\n",
      "Epoch 19/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 1.6629 - accuracy: 0.4363 - val_loss: 1.7105 - val_accuracy: 0.4096\n",
      "Epoch 20/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 1.6605 - accuracy: 0.4384 - val_loss: 1.7081 - val_accuracy: 0.4093\n",
      "Epoch 21/70\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 1.6585 - accuracy: 0.4385 - val_loss: 1.7150 - val_accuracy: 0.4068\n",
      "Epoch 22/70\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 1.6573 - accuracy: 0.4385 - val_loss: 1.7073 - val_accuracy: 0.4132\n",
      "Epoch 23/70\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 1.6551 - accuracy: 0.4389 - val_loss: 1.7061 - val_accuracy: 0.4150\n",
      "Epoch 24/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 1.6526 - accuracy: 0.4412 - val_loss: 1.7117 - val_accuracy: 0.4112\n",
      "Epoch 25/70\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 1.6516 - accuracy: 0.4408 - val_loss: 1.7095 - val_accuracy: 0.4136\n",
      "Epoch 26/70\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 1.6502 - accuracy: 0.4418 - val_loss: 1.7114 - val_accuracy: 0.4116\n",
      "Epoch 27/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 1.6491 - accuracy: 0.4415 - val_loss: 1.7073 - val_accuracy: 0.4118\n",
      "Epoch 28/70\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 1.6478 - accuracy: 0.4421 - val_loss: 1.7104 - val_accuracy: 0.4098\n",
      "Epoch 29/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 1.6463 - accuracy: 0.4430 - val_loss: 1.7087 - val_accuracy: 0.4117\n",
      "Epoch 30/70\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 1.6447 - accuracy: 0.4440 - val_loss: 1.7072 - val_accuracy: 0.4118\n",
      "Epoch 31/70\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 1.6437 - accuracy: 0.4441 - val_loss: 1.7071 - val_accuracy: 0.4127\n",
      "Epoch 32/70\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 1.6424 - accuracy: 0.4442 - val_loss: 1.7103 - val_accuracy: 0.4113\n",
      "Epoch 33/70\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 1.6416 - accuracy: 0.4448 - val_loss: 1.7111 - val_accuracy: 0.4095\n",
      "Epoch 34/70\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 1.6407 - accuracy: 0.4450 - val_loss: 1.7114 - val_accuracy: 0.4097\n",
      "Epoch 35/70\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 1.6393 - accuracy: 0.4441 - val_loss: 1.7131 - val_accuracy: 0.4118\n",
      "Epoch 36/70\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 1.6385 - accuracy: 0.4455 - val_loss: 1.7130 - val_accuracy: 0.4113\n",
      "Epoch 37/70\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 1.6381 - accuracy: 0.4465 - val_loss: 1.7076 - val_accuracy: 0.4158\n",
      "Epoch 38/70\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 1.6374 - accuracy: 0.4460 - val_loss: 1.7088 - val_accuracy: 0.4122\n",
      "Epoch 39/70\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 1.6353 - accuracy: 0.4470 - val_loss: 1.7121 - val_accuracy: 0.4093\n",
      "Epoch 40/70\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 1.6354 - accuracy: 0.4479 - val_loss: 1.7127 - val_accuracy: 0.4104\n",
      "Epoch 41/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 1.6341 - accuracy: 0.4470 - val_loss: 1.7124 - val_accuracy: 0.4115\n",
      "Epoch 42/70\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 1.6339 - accuracy: 0.4461 - val_loss: 1.7104 - val_accuracy: 0.4140\n",
      "Epoch 43/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 1.6329 - accuracy: 0.4466 - val_loss: 1.7131 - val_accuracy: 0.4123\n",
      "Epoch 44/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 1.6319 - accuracy: 0.4492 - val_loss: 1.7132 - val_accuracy: 0.4088\n",
      "Epoch 45/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 1.6311 - accuracy: 0.4471 - val_loss: 1.7146 - val_accuracy: 0.4106\n",
      "Epoch 46/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 1.6307 - accuracy: 0.4481 - val_loss: 1.7109 - val_accuracy: 0.4127\n",
      "Epoch 47/70\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 1.6298 - accuracy: 0.4498 - val_loss: 1.7140 - val_accuracy: 0.4123\n",
      "Epoch 48/70\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 1.6289 - accuracy: 0.4506 - val_loss: 1.7124 - val_accuracy: 0.4105\n",
      "Epoch 49/70\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 1.6282 - accuracy: 0.4500 - val_loss: 1.7138 - val_accuracy: 0.4106\n",
      "Epoch 50/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 1.6275 - accuracy: 0.4503 - val_loss: 1.7146 - val_accuracy: 0.4094\n",
      "Epoch 51/70\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 1.6269 - accuracy: 0.4508 - val_loss: 1.7134 - val_accuracy: 0.4096\n",
      "Epoch 52/70\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 1.6262 - accuracy: 0.4496 - val_loss: 1.7133 - val_accuracy: 0.4107\n",
      "Epoch 53/70\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 1.6255 - accuracy: 0.4507 - val_loss: 1.7157 - val_accuracy: 0.4091\n",
      "Epoch 54/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 1.6248 - accuracy: 0.4512 - val_loss: 1.7169 - val_accuracy: 0.4083\n",
      "Epoch 55/70\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 1.6245 - accuracy: 0.4508 - val_loss: 1.7139 - val_accuracy: 0.4094\n",
      "Epoch 56/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 1.6237 - accuracy: 0.4512 - val_loss: 1.7215 - val_accuracy: 0.4056\n",
      "Epoch 57/70\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 1.6237 - accuracy: 0.4515 - val_loss: 1.7134 - val_accuracy: 0.4124\n",
      "Epoch 58/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 1s 3ms/step - loss: 1.6230 - accuracy: 0.4519 - val_loss: 1.7164 - val_accuracy: 0.4119\n",
      "Epoch 59/70\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 1.6219 - accuracy: 0.4523 - val_loss: 1.7144 - val_accuracy: 0.4118\n",
      "Epoch 60/70\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 1.6218 - accuracy: 0.4502 - val_loss: 1.7154 - val_accuracy: 0.4128\n",
      "Epoch 61/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 1.6212 - accuracy: 0.4518 - val_loss: 1.7164 - val_accuracy: 0.4077\n",
      "Epoch 62/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 1.6203 - accuracy: 0.4524 - val_loss: 1.7176 - val_accuracy: 0.4100\n",
      "Epoch 63/70\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 1.6198 - accuracy: 0.4533 - val_loss: 1.7167 - val_accuracy: 0.4116\n",
      "Epoch 64/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 1.6195 - accuracy: 0.4538 - val_loss: 1.7182 - val_accuracy: 0.4111\n",
      "Epoch 65/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 1.6188 - accuracy: 0.4529 - val_loss: 1.7200 - val_accuracy: 0.4062\n",
      "Epoch 66/70\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 1.6183 - accuracy: 0.4536 - val_loss: 1.7163 - val_accuracy: 0.4101\n",
      "Epoch 67/70\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 1.6180 - accuracy: 0.4532 - val_loss: 1.7141 - val_accuracy: 0.4105\n",
      "Epoch 68/70\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 1.6170 - accuracy: 0.4546 - val_loss: 1.7164 - val_accuracy: 0.4087\n",
      "Epoch 69/70\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 1.6173 - accuracy: 0.4525 - val_loss: 1.7158 - val_accuracy: 0.4113\n",
      "Epoch 70/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 1.6161 - accuracy: 0.4548 - val_loss: 1.7188 - val_accuracy: 0.4091\n"
     ]
    }
   ],
   "source": [
    "softmax_regression = models.Sequential([\n",
    "    layers.Flatten(input_shape=image_shape),\n",
    "    layers.Dense(10, activation='softmax')],\n",
    "    name='linear_adam')\n",
    "train_model(softmax_regression, optimizer=optimizers.Adam, learning_rate=2e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f97936a20e0833a4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Interpreting the Learned Weights\n",
    "\n",
    "Multiplication by the weights $W$ can be interpreted as computing responses to correlation templates per image class.\n",
    "\n",
    "That means, we can reshape the weight array $W$ to a obtain \"template images\".\n",
    "\n",
    "Perform this reshaping and visualize the resulting templates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-6b90988a89171165",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3072, 10)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x288 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "W, b = softmax_regression.layers[1].get_weights()\n",
    "\n",
    "print(W.shape)\n",
    "# Create the `templates` variable here based on W, with dimensions [10 (class count), height, width, 3 (rgb)]\n",
    "### BEGIN SOLUTION\n",
    "templates = W.reshape(32,32,3,10).transpose(3,0,1,2)\n",
    "### END SOLUTION\n",
    "\n",
    "# We normalize the templates to the 0-1 range for visualization\n",
    "mini = np.min(templates, axis=(1,2,3), keepdims=True)\n",
    "maxi = np.max(templates, axis=(1,2,3), keepdims=True)\n",
    "rescaled_templates = (templates - mini)/ (maxi-mini)\n",
    "plot_multiple(rescaled_templates, labels, max_columns=5, imwidth=2, imheight=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a48193761ec01321",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Do they look as you would expect?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-82b5467968214e70",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "Some patterns are visible: frogs are green in the center, horses have large red values in the upper center of the image and have a green spot below. Ship images have blue pixels in the bottom corners, deers are surrounded by green."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-53dcad4765f2ab5b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Multi-Layer Perceptron\n",
    "\n",
    "Softmax regression has a big limitation: the decision surface between any two classes (i.e. the part of the input space where the classification decision changes from one class to another) is a simple hyperplane (\"flat\").\n",
    "\n",
    "The **multi-layer perceptron** (MLP) is a neural network model with additional layer(s) between the input and the logits (so-called hidden layers), with nonlinear activation functions. Why are activation functions needed?\n",
    "\n",
    "Before the current generation of neural networks, the **hyperbolic tangent** (tanh) function used to be the preferred activation function in the hidden layers of MLPs. It is sigmoid shaped and has a range of $(-1,1)$. We can create such a network in Keras as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-17fa5a99f79769c6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/70\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 1.8107 - accuracy: 0.3740 - val_loss: 1.7244 - val_accuracy: 0.4046\n",
      "Epoch 2/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 1.6787 - accuracy: 0.4243 - val_loss: 1.6859 - val_accuracy: 0.4112\n",
      "Epoch 3/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 1.6143 - accuracy: 0.4478 - val_loss: 1.6294 - val_accuracy: 0.4406\n",
      "Epoch 4/70\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 1.5613 - accuracy: 0.4660 - val_loss: 1.5986 - val_accuracy: 0.4447\n",
      "Epoch 5/70\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 1.5142 - accuracy: 0.4859 - val_loss: 1.5800 - val_accuracy: 0.4566\n",
      "Epoch 6/70\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 1.4723 - accuracy: 0.5022 - val_loss: 1.5648 - val_accuracy: 0.4616\n",
      "Epoch 7/70\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 1.4331 - accuracy: 0.5174 - val_loss: 1.5377 - val_accuracy: 0.4727\n",
      "Epoch 8/70\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 1.3923 - accuracy: 0.5298 - val_loss: 1.5496 - val_accuracy: 0.4681\n",
      "Epoch 9/70\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 1.3574 - accuracy: 0.5429 - val_loss: 1.5163 - val_accuracy: 0.4813\n",
      "Epoch 10/70\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 1.3201 - accuracy: 0.5563 - val_loss: 1.5013 - val_accuracy: 0.4869\n",
      "Epoch 11/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 1.2854 - accuracy: 0.5702 - val_loss: 1.4974 - val_accuracy: 0.4869\n",
      "Epoch 12/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 1.2497 - accuracy: 0.5844 - val_loss: 1.4944 - val_accuracy: 0.4868\n",
      "Epoch 13/70\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 1.2169 - accuracy: 0.5960 - val_loss: 1.4905 - val_accuracy: 0.4868\n",
      "Epoch 14/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 1.1821 - accuracy: 0.6105 - val_loss: 1.4859 - val_accuracy: 0.4948\n",
      "Epoch 15/70\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 1.1497 - accuracy: 0.6223 - val_loss: 1.4756 - val_accuracy: 0.4892\n",
      "Epoch 16/70\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 1.1168 - accuracy: 0.6342 - val_loss: 1.4720 - val_accuracy: 0.4993\n",
      "Epoch 17/70\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 1.0847 - accuracy: 0.6473 - val_loss: 1.4794 - val_accuracy: 0.4928\n",
      "Epoch 18/70\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 1.0526 - accuracy: 0.6595 - val_loss: 1.4783 - val_accuracy: 0.4915\n",
      "Epoch 19/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 1.0215 - accuracy: 0.6722 - val_loss: 1.4744 - val_accuracy: 0.4991\n",
      "Epoch 20/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.9927 - accuracy: 0.6832 - val_loss: 1.4676 - val_accuracy: 0.5011\n",
      "Epoch 21/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.9612 - accuracy: 0.6964 - val_loss: 1.4761 - val_accuracy: 0.5002\n",
      "Epoch 22/70\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 0.9317 - accuracy: 0.7069 - val_loss: 1.4716 - val_accuracy: 0.4988\n",
      "Epoch 23/70\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 0.9007 - accuracy: 0.7183 - val_loss: 1.4868 - val_accuracy: 0.5005\n",
      "Epoch 24/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.8736 - accuracy: 0.7302 - val_loss: 1.4802 - val_accuracy: 0.5002\n",
      "Epoch 25/70\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 0.8457 - accuracy: 0.7409 - val_loss: 1.4930 - val_accuracy: 0.5003\n",
      "Epoch 26/70\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 0.8179 - accuracy: 0.7515 - val_loss: 1.4800 - val_accuracy: 0.5032\n",
      "Epoch 27/70\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 0.7901 - accuracy: 0.7633 - val_loss: 1.5005 - val_accuracy: 0.4993\n",
      "Epoch 28/70\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 0.7622 - accuracy: 0.7737 - val_loss: 1.5004 - val_accuracy: 0.4973\n",
      "Epoch 29/70\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 0.7370 - accuracy: 0.7834 - val_loss: 1.5135 - val_accuracy: 0.4980\n",
      "Epoch 30/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.7107 - accuracy: 0.7929 - val_loss: 1.5140 - val_accuracy: 0.5013\n",
      "Epoch 31/70\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 0.6880 - accuracy: 0.8009 - val_loss: 1.5246 - val_accuracy: 0.5002\n",
      "Epoch 32/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.6634 - accuracy: 0.8115 - val_loss: 1.5317 - val_accuracy: 0.4967\n",
      "Epoch 33/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.6407 - accuracy: 0.8177 - val_loss: 1.5389 - val_accuracy: 0.4977\n",
      "Epoch 34/70\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 0.6163 - accuracy: 0.8299 - val_loss: 1.5534 - val_accuracy: 0.4944\n",
      "Epoch 35/70\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 0.5951 - accuracy: 0.8372 - val_loss: 1.5488 - val_accuracy: 0.4984\n",
      "Epoch 36/70\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 0.5694 - accuracy: 0.8492 - val_loss: 1.5539 - val_accuracy: 0.4993\n",
      "Epoch 37/70\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 0.5484 - accuracy: 0.8550 - val_loss: 1.5714 - val_accuracy: 0.5020\n",
      "Epoch 38/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.5302 - accuracy: 0.8614 - val_loss: 1.5807 - val_accuracy: 0.4987\n",
      "Epoch 39/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.5096 - accuracy: 0.8698 - val_loss: 1.6147 - val_accuracy: 0.4946\n",
      "Epoch 40/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.4896 - accuracy: 0.8751 - val_loss: 1.6133 - val_accuracy: 0.4928\n",
      "Epoch 41/70\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 0.4702 - accuracy: 0.8835 - val_loss: 1.6160 - val_accuracy: 0.4983\n",
      "Epoch 42/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.4497 - accuracy: 0.8905 - val_loss: 1.6365 - val_accuracy: 0.4934\n",
      "Epoch 43/70\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 0.4335 - accuracy: 0.8969 - val_loss: 1.6464 - val_accuracy: 0.4934\n",
      "Epoch 44/70\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 0.4151 - accuracy: 0.9024 - val_loss: 1.6528 - val_accuracy: 0.4925\n",
      "Epoch 45/70\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 0.3969 - accuracy: 0.9102 - val_loss: 1.6636 - val_accuracy: 0.4955\n",
      "Epoch 46/70\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 0.3819 - accuracy: 0.9147 - val_loss: 1.6920 - val_accuracy: 0.4941\n",
      "Epoch 47/70\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 0.3661 - accuracy: 0.9199 - val_loss: 1.7070 - val_accuracy: 0.4900\n",
      "Epoch 48/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.3486 - accuracy: 0.9257 - val_loss: 1.7191 - val_accuracy: 0.4854\n",
      "Epoch 49/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.3338 - accuracy: 0.9302 - val_loss: 1.7252 - val_accuracy: 0.4899\n",
      "Epoch 50/70\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 0.3188 - accuracy: 0.9357 - val_loss: 1.7459 - val_accuracy: 0.4894\n",
      "Epoch 51/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.3054 - accuracy: 0.9399 - val_loss: 1.7577 - val_accuracy: 0.4904\n",
      "Epoch 52/70\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 0.2926 - accuracy: 0.9441 - val_loss: 1.7740 - val_accuracy: 0.4875\n",
      "Epoch 53/70\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 0.2778 - accuracy: 0.9492 - val_loss: 1.7986 - val_accuracy: 0.4884\n",
      "Epoch 54/70\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 0.2667 - accuracy: 0.9513 - val_loss: 1.7990 - val_accuracy: 0.4928\n",
      "Epoch 55/70\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 0.2547 - accuracy: 0.9558 - val_loss: 1.8289 - val_accuracy: 0.4860\n",
      "Epoch 56/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.2412 - accuracy: 0.9602 - val_loss: 1.8510 - val_accuracy: 0.4852\n",
      "Epoch 57/70\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 0.2322 - accuracy: 0.9619 - val_loss: 1.8478 - val_accuracy: 0.4871\n",
      "Epoch 58/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 2s 4ms/step - loss: 0.2201 - accuracy: 0.9651 - val_loss: 1.8691 - val_accuracy: 0.4891\n",
      "Epoch 59/70\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 0.2083 - accuracy: 0.9684 - val_loss: 1.8783 - val_accuracy: 0.4917\n",
      "Epoch 60/70\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 0.1991 - accuracy: 0.9706 - val_loss: 1.9111 - val_accuracy: 0.4876\n",
      "Epoch 61/70\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 0.1896 - accuracy: 0.9730 - val_loss: 1.9230 - val_accuracy: 0.4861\n",
      "Epoch 62/70\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 0.1816 - accuracy: 0.9746 - val_loss: 1.9351 - val_accuracy: 0.4866\n",
      "Epoch 63/70\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 0.1704 - accuracy: 0.9772 - val_loss: 1.9625 - val_accuracy: 0.4781\n",
      "Epoch 64/70\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 0.1632 - accuracy: 0.9787 - val_loss: 1.9810 - val_accuracy: 0.4870\n",
      "Epoch 65/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.1553 - accuracy: 0.9805 - val_loss: 1.9932 - val_accuracy: 0.4856\n",
      "Epoch 66/70\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 0.1460 - accuracy: 0.9825 - val_loss: 2.0109 - val_accuracy: 0.4834\n",
      "Epoch 67/70\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 0.1404 - accuracy: 0.9835 - val_loss: 2.0392 - val_accuracy: 0.4828\n",
      "Epoch 68/70\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 0.1313 - accuracy: 0.9860 - val_loss: 2.0471 - val_accuracy: 0.4788\n",
      "Epoch 69/70\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 0.1256 - accuracy: 0.9864 - val_loss: 2.0736 - val_accuracy: 0.4808\n",
      "Epoch 70/70\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 0.1198 - accuracy: 0.9872 - val_loss: 2.0815 - val_accuracy: 0.4842\n"
     ]
    }
   ],
   "source": [
    "tanh_mlp = models.Sequential([\n",
    "    layers.Flatten(input_shape=image_shape),\n",
    "    layers.Dense(512, activation='tanh'),\n",
    "    layers.Dense(10, activation='softmax')],\n",
    "    name='tanh_mlp')\n",
    "\n",
    "train_model(tanh_mlp, optimizer=optimizers.Adam, learning_rate=2e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2b139794e1c54e5e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Does it obtain better results than the linear model? What do you observe if you compare the curves for training and validation?\n",
    "\n",
    "How and why does the behaviour of the validation loss differ from the validation accuracy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-9a16b9da4520bc8b",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "The validation accuracy gets a bit better compared to the previous model. The training accuracy goes close to 1, but the validation accuracy starts to decrease after about 30 epochs, thus the model overfits to the training data.\n",
    "\n",
    "After 20 epochs, the validation loss gets worse and after 45 epochs the validation loss is even lower compared to the initialized model and it continues the grow. In contrast, the validation accuracy decreases only by a few percent. The latter only counts the number of correctly classified classes, so if the model predicts the correct class with confidence 0.11 and all others with slightly less, the accuracy can still be high. The cross-entropy loss however punishes low confidences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-52198a15ed79a2ee",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## ReLU\n",
    "\n",
    "The ReLU activation function has become more popular in recent years, especially for deeper nets. Create and train an MLP that uses ReLU as the activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-209c6c6b658fdc85",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/70\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.6566 - accuracy: 0.4223 - val_loss: 1.5319 - val_accuracy: 0.4668\n",
      "Epoch 2/70\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 1.4636 - accuracy: 0.4951 - val_loss: 1.4557 - val_accuracy: 0.4932\n",
      "Epoch 3/70\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 1.3688 - accuracy: 0.5295 - val_loss: 1.4099 - val_accuracy: 0.5088\n",
      "Epoch 4/70\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 1.2978 - accuracy: 0.5571 - val_loss: 1.3974 - val_accuracy: 0.5079\n",
      "Epoch 5/70\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 1.2433 - accuracy: 0.5762 - val_loss: 1.3599 - val_accuracy: 0.5249\n",
      "Epoch 6/70\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 1.1950 - accuracy: 0.5959 - val_loss: 1.3667 - val_accuracy: 0.5198\n",
      "Epoch 7/70\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 1.1491 - accuracy: 0.6128 - val_loss: 1.3402 - val_accuracy: 0.5344\n",
      "Epoch 8/70\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 1.1056 - accuracy: 0.6279 - val_loss: 1.3228 - val_accuracy: 0.5410\n",
      "Epoch 9/70\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 1.0674 - accuracy: 0.6418 - val_loss: 1.3253 - val_accuracy: 0.5370\n",
      "Epoch 10/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 1.0349 - accuracy: 0.6537 - val_loss: 1.3211 - val_accuracy: 0.5362\n",
      "Epoch 11/70\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 0.9975 - accuracy: 0.6693 - val_loss: 1.3113 - val_accuracy: 0.5439\n",
      "Epoch 12/70\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 0.9630 - accuracy: 0.6813 - val_loss: 1.3186 - val_accuracy: 0.5383\n",
      "Epoch 13/70\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 0.9303 - accuracy: 0.6931 - val_loss: 1.3219 - val_accuracy: 0.5401\n",
      "Epoch 14/70\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 0.9018 - accuracy: 0.7026 - val_loss: 1.3095 - val_accuracy: 0.5439\n",
      "Epoch 15/70\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 0.8723 - accuracy: 0.7126 - val_loss: 1.3311 - val_accuracy: 0.5434\n",
      "Epoch 16/70\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 0.8426 - accuracy: 0.7257 - val_loss: 1.3320 - val_accuracy: 0.5441\n",
      "Epoch 17/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.8125 - accuracy: 0.7380 - val_loss: 1.3151 - val_accuracy: 0.5537\n",
      "Epoch 18/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.7871 - accuracy: 0.7472 - val_loss: 1.3404 - val_accuracy: 0.5426\n",
      "Epoch 19/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.7599 - accuracy: 0.7567 - val_loss: 1.3347 - val_accuracy: 0.5470\n",
      "Epoch 20/70\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 0.7366 - accuracy: 0.7663 - val_loss: 1.3355 - val_accuracy: 0.5472\n",
      "Epoch 21/70\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 0.7111 - accuracy: 0.7765 - val_loss: 1.3550 - val_accuracy: 0.5464\n",
      "Epoch 22/70\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 0.6892 - accuracy: 0.7823 - val_loss: 1.3669 - val_accuracy: 0.5474\n",
      "Epoch 23/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.6652 - accuracy: 0.7933 - val_loss: 1.3570 - val_accuracy: 0.5487\n",
      "Epoch 24/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.6453 - accuracy: 0.8010 - val_loss: 1.3777 - val_accuracy: 0.5440\n",
      "Epoch 25/70\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 0.6207 - accuracy: 0.8110 - val_loss: 1.3902 - val_accuracy: 0.5496\n",
      "Epoch 26/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.5991 - accuracy: 0.8208 - val_loss: 1.3889 - val_accuracy: 0.5449\n",
      "Epoch 27/70\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 0.5794 - accuracy: 0.8266 - val_loss: 1.4091 - val_accuracy: 0.5441\n",
      "Epoch 28/70\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 0.5592 - accuracy: 0.8336 - val_loss: 1.4144 - val_accuracy: 0.5430\n",
      "Epoch 29/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.5390 - accuracy: 0.8428 - val_loss: 1.4296 - val_accuracy: 0.5475\n",
      "Epoch 30/70\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 0.5227 - accuracy: 0.8476 - val_loss: 1.4415 - val_accuracy: 0.5417\n",
      "Epoch 31/70\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 0.5053 - accuracy: 0.8547 - val_loss: 1.4527 - val_accuracy: 0.5404\n",
      "Epoch 32/70\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 0.4863 - accuracy: 0.8618 - val_loss: 1.4359 - val_accuracy: 0.5468\n",
      "Epoch 33/70\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 0.4688 - accuracy: 0.8679 - val_loss: 1.4699 - val_accuracy: 0.5459\n",
      "Epoch 34/70\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 0.4494 - accuracy: 0.8764 - val_loss: 1.4865 - val_accuracy: 0.5379\n",
      "Epoch 35/70\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 0.4351 - accuracy: 0.8806 - val_loss: 1.4918 - val_accuracy: 0.5462\n",
      "Epoch 36/70\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 0.4194 - accuracy: 0.8857 - val_loss: 1.5008 - val_accuracy: 0.5411\n",
      "Epoch 37/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.4025 - accuracy: 0.8919 - val_loss: 1.5240 - val_accuracy: 0.5460\n",
      "Epoch 38/70\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 0.3897 - accuracy: 0.8967 - val_loss: 1.5229 - val_accuracy: 0.5409\n",
      "Epoch 39/70\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 0.3781 - accuracy: 0.9014 - val_loss: 1.5469 - val_accuracy: 0.5422\n",
      "Epoch 40/70\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 0.3581 - accuracy: 0.9090 - val_loss: 1.5449 - val_accuracy: 0.5474\n",
      "Epoch 41/70\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 0.3460 - accuracy: 0.9131 - val_loss: 1.5715 - val_accuracy: 0.5400\n",
      "Epoch 42/70\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 0.3342 - accuracy: 0.9170 - val_loss: 1.5846 - val_accuracy: 0.5426\n",
      "Epoch 43/70\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 0.3196 - accuracy: 0.9240 - val_loss: 1.5970 - val_accuracy: 0.5378\n",
      "Epoch 44/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.3097 - accuracy: 0.9258 - val_loss: 1.6160 - val_accuracy: 0.5400\n",
      "Epoch 45/70\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 0.2953 - accuracy: 0.9305 - val_loss: 1.6253 - val_accuracy: 0.5398\n",
      "Epoch 46/70\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 0.2844 - accuracy: 0.9341 - val_loss: 1.6690 - val_accuracy: 0.5304\n",
      "Epoch 47/70\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 0.2718 - accuracy: 0.9385 - val_loss: 1.6705 - val_accuracy: 0.5329\n",
      "Epoch 48/70\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 0.2622 - accuracy: 0.9414 - val_loss: 1.6742 - val_accuracy: 0.5425\n",
      "Epoch 49/70\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 0.2558 - accuracy: 0.9426 - val_loss: 1.7309 - val_accuracy: 0.5340\n",
      "Epoch 50/70\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 0.2442 - accuracy: 0.9472 - val_loss: 1.7233 - val_accuracy: 0.5314\n",
      "Epoch 51/70\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 0.2344 - accuracy: 0.9495 - val_loss: 1.7346 - val_accuracy: 0.5377\n",
      "Epoch 52/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.2203 - accuracy: 0.9559 - val_loss: 1.7360 - val_accuracy: 0.5382\n",
      "Epoch 53/70\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 0.2120 - accuracy: 0.9581 - val_loss: 1.7538 - val_accuracy: 0.5382\n",
      "Epoch 54/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.2077 - accuracy: 0.9571 - val_loss: 1.7804 - val_accuracy: 0.5374\n",
      "Epoch 55/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.1976 - accuracy: 0.9615 - val_loss: 1.7869 - val_accuracy: 0.5353\n",
      "Epoch 56/70\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 0.1885 - accuracy: 0.9647 - val_loss: 1.8328 - val_accuracy: 0.5304\n",
      "Epoch 57/70\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 0.1812 - accuracy: 0.9660 - val_loss: 1.8174 - val_accuracy: 0.5412\n",
      "Epoch 58/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 2s 4ms/step - loss: 0.1744 - accuracy: 0.9682 - val_loss: 1.8478 - val_accuracy: 0.5340\n",
      "Epoch 59/70\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 0.1700 - accuracy: 0.9690 - val_loss: 1.8713 - val_accuracy: 0.5308\n",
      "Epoch 60/70\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 0.1612 - accuracy: 0.9721 - val_loss: 1.8732 - val_accuracy: 0.5341\n",
      "Epoch 61/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.1547 - accuracy: 0.9733 - val_loss: 1.8941 - val_accuracy: 0.5355\n",
      "Epoch 62/70\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 0.1440 - accuracy: 0.9774 - val_loss: 1.9295 - val_accuracy: 0.5298\n",
      "Epoch 63/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.1473 - accuracy: 0.9736 - val_loss: 1.9839 - val_accuracy: 0.5349\n",
      "Epoch 64/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.1353 - accuracy: 0.9776 - val_loss: 1.9559 - val_accuracy: 0.5292\n",
      "Epoch 65/70\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 0.1250 - accuracy: 0.9813 - val_loss: 1.9895 - val_accuracy: 0.5292\n",
      "Epoch 66/70\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 0.1270 - accuracy: 0.9802 - val_loss: 1.9834 - val_accuracy: 0.5274\n",
      "Epoch 67/70\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 0.1196 - accuracy: 0.9819 - val_loss: 2.0489 - val_accuracy: 0.5270\n",
      "Epoch 68/70\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 0.1152 - accuracy: 0.9826 - val_loss: 2.0359 - val_accuracy: 0.5282\n",
      "Epoch 69/70\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 0.1087 - accuracy: 0.9847 - val_loss: 2.0465 - val_accuracy: 0.5286\n",
      "Epoch 70/70\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 0.1053 - accuracy: 0.9854 - val_loss: 2.0582 - val_accuracy: 0.5310\n"
     ]
    }
   ],
   "source": [
    "relu_mlp = models.Sequential([\n",
    "    layers.Flatten(input_shape=image_shape),\n",
    "    layers.Dense(512, activation='relu', kernel_initializer='he_uniform'),\n",
    "    layers.Dense(10, activation='softmax')],\n",
    "    name='relu_mlp')\n",
    "train_model(relu_mlp, optimizer=optimizers.Adam, learning_rate=2e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b2c3bfe1f9ceab2f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Do the results change? What benefits does ReLU have against tanh?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-e96f4ec268fbbe6c",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "The results again get better. Due to the linear behaviour of ReLU for positive values, it has less issues with vanishing gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3c893def9eef0d29",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## A Simple Convolutional Neural Network\n",
    "\n",
    "The previous models did not explicitly make use of the grid structure of the image pixels. Convolutional neural networks do.\n",
    "\n",
    "Instead of reshaping the input image pixels into one long vector, convolutional layers slide small filters across the input, just as with the convolutional filters we saw earlier in the course. In the earlier parts, we looked at convolution on an image with a single channel in case of grayscale images, or channelwise separate convolutions on RGB images.\n",
    "\n",
    "In CNNs, the multiple input channels of a conv layer are not handled independently, but are linearly combined. This means that the weight array has shape `[kernel_height, kernel_width, num_input_channels, num_output_channels]` and we perform a weighted sum along the input channel axis. Another difference is the use of a **bias** vector of shape `[num_output_channels]`, each component of which gets added on the corresponding output channel.\n",
    "\n",
    "As you already know, convolution is a linear operator, so it is possible to express any convolutional layer as a fully-connected layer.\n",
    "However, the convolutional layer's weight matrix is sparse (has many zeros) compared to a fully-connected (\"dense\") layer because each output only depends on a small number of inputs, namely, those within a small neigborhood. Further, the weight values are shared between the different pixel locations.\n",
    "\n",
    "This tutorial has some great visualisations and explanations on the details of conv layers: https://arxiv.org/abs/1603.07285.\n",
    "\n",
    "Assuming a fixed input image size, do you think the reverse of the above also holds? Can any fully-connected layer be expressed as a convolutional layer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-a43655d6b36f378f",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "Yes, the trick is that the convolutional filter needs to have the size of the entire input image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d5bd65e1aa6cf240",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Technically, what's called a \"convolutional\" layer is usually implemented as a *cross-correlation* computation. Could there be any advantage in using the actual definition of convolution in these layers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-9f6f8235182c9277",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "No. Since the weights of a filter kernel are all sampled from the same random distribution in the beginning of training, flipping the kernel would make no systematic difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8861acb4ed9aa147",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Train the following simple CNN model. It may take about 15 minutes on a CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d01c8cd05d41bef2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/70\n",
      "391/391 [==============================] - 43s 111ms/step - loss: 1.4053 - accuracy: 0.5046 - val_loss: 1.1607 - val_accuracy: 0.5898\n",
      "Epoch 2/70\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 1.0312 - accuracy: 0.6438 - val_loss: 0.9776 - val_accuracy: 0.6578\n",
      "Epoch 3/70\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.9046 - accuracy: 0.6871 - val_loss: 0.9275 - val_accuracy: 0.6784\n",
      "Epoch 4/70\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.8197 - accuracy: 0.7203 - val_loss: 0.9026 - val_accuracy: 0.6952\n",
      "Epoch 5/70\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.7517 - accuracy: 0.7413 - val_loss: 0.8430 - val_accuracy: 0.7133\n",
      "Epoch 6/70\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.6960 - accuracy: 0.7596 - val_loss: 0.8263 - val_accuracy: 0.7189\n",
      "Epoch 7/70\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.6493 - accuracy: 0.7756 - val_loss: 0.8230 - val_accuracy: 0.7145\n",
      "Epoch 8/70\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.6098 - accuracy: 0.7927 - val_loss: 0.8377 - val_accuracy: 0.7191\n",
      "Epoch 9/70\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.5710 - accuracy: 0.8031 - val_loss: 0.8402 - val_accuracy: 0.7209\n",
      "Epoch 10/70\n",
      "391/391 [==============================] - 3s 6ms/step - loss: 0.5427 - accuracy: 0.8141 - val_loss: 0.8274 - val_accuracy: 0.7232\n",
      "Epoch 11/70\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.5091 - accuracy: 0.8260 - val_loss: 0.8653 - val_accuracy: 0.7241\n",
      "Epoch 12/70\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.4789 - accuracy: 0.8337 - val_loss: 0.9170 - val_accuracy: 0.7170\n",
      "Epoch 13/70\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.4571 - accuracy: 0.8447 - val_loss: 0.8763 - val_accuracy: 0.7269\n",
      "Epoch 14/70\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.4295 - accuracy: 0.8531 - val_loss: 0.9173 - val_accuracy: 0.7198\n",
      "Epoch 15/70\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.4108 - accuracy: 0.8590 - val_loss: 0.9440 - val_accuracy: 0.7147\n",
      "Epoch 16/70\n",
      "391/391 [==============================] - 3s 6ms/step - loss: 0.3881 - accuracy: 0.8670 - val_loss: 0.9499 - val_accuracy: 0.7180\n",
      "Epoch 17/70\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.3677 - accuracy: 0.8733 - val_loss: 0.9703 - val_accuracy: 0.7162\n",
      "Epoch 18/70\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.3464 - accuracy: 0.8815 - val_loss: 0.9716 - val_accuracy: 0.7225\n",
      "Epoch 19/70\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.3310 - accuracy: 0.8861 - val_loss: 1.0183 - val_accuracy: 0.7201\n",
      "Epoch 20/70\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.3127 - accuracy: 0.8930 - val_loss: 1.0263 - val_accuracy: 0.7175\n",
      "Epoch 21/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.3005 - accuracy: 0.8967 - val_loss: 1.0619 - val_accuracy: 0.7175\n",
      "Epoch 22/70\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.2812 - accuracy: 0.9040 - val_loss: 1.0786 - val_accuracy: 0.7152\n",
      "Epoch 23/70\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.2670 - accuracy: 0.9103 - val_loss: 1.1173 - val_accuracy: 0.7138\n",
      "Epoch 24/70\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2527 - accuracy: 0.9140 - val_loss: 1.1707 - val_accuracy: 0.7108\n",
      "Epoch 25/70\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.2394 - accuracy: 0.9193 - val_loss: 1.1733 - val_accuracy: 0.7120\n",
      "Epoch 26/70\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.2272 - accuracy: 0.9229 - val_loss: 1.1987 - val_accuracy: 0.7120\n",
      "Epoch 27/70\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.2177 - accuracy: 0.9253 - val_loss: 1.2874 - val_accuracy: 0.7074\n",
      "Epoch 28/70\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.2028 - accuracy: 0.9318 - val_loss: 1.2591 - val_accuracy: 0.7104\n",
      "Epoch 29/70\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.1912 - accuracy: 0.9344 - val_loss: 1.3669 - val_accuracy: 0.6956\n",
      "Epoch 30/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.1892 - accuracy: 0.9354 - val_loss: 1.3504 - val_accuracy: 0.7043\n",
      "Epoch 31/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.1725 - accuracy: 0.9424 - val_loss: 1.3917 - val_accuracy: 0.7047\n",
      "Epoch 32/70\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.1594 - accuracy: 0.9477 - val_loss: 1.4103 - val_accuracy: 0.7094\n",
      "Epoch 33/70\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.1535 - accuracy: 0.9477 - val_loss: 1.4651 - val_accuracy: 0.7090\n",
      "Epoch 34/70\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.1508 - accuracy: 0.9500 - val_loss: 1.5239 - val_accuracy: 0.7030\n",
      "Epoch 35/70\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.1386 - accuracy: 0.9541 - val_loss: 1.5302 - val_accuracy: 0.7075\n",
      "Epoch 36/70\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.1275 - accuracy: 0.9580 - val_loss: 1.5772 - val_accuracy: 0.7036\n",
      "Epoch 37/70\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.1255 - accuracy: 0.9582 - val_loss: 1.6581 - val_accuracy: 0.7007\n",
      "Epoch 38/70\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.1180 - accuracy: 0.9610 - val_loss: 1.6571 - val_accuracy: 0.7028\n",
      "Epoch 39/70\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.1152 - accuracy: 0.9613 - val_loss: 1.7183 - val_accuracy: 0.6991\n",
      "Epoch 40/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.1070 - accuracy: 0.9656 - val_loss: 1.7956 - val_accuracy: 0.6939\n",
      "Epoch 41/70\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.1064 - accuracy: 0.9648 - val_loss: 1.7254 - val_accuracy: 0.7017\n",
      "Epoch 42/70\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0971 - accuracy: 0.9682 - val_loss: 1.8252 - val_accuracy: 0.7004\n",
      "Epoch 43/70\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0843 - accuracy: 0.9742 - val_loss: 1.8566 - val_accuracy: 0.6995\n",
      "Epoch 44/70\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.0840 - accuracy: 0.9741 - val_loss: 1.8882 - val_accuracy: 0.6966\n",
      "Epoch 45/70\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0832 - accuracy: 0.9725 - val_loss: 1.9073 - val_accuracy: 0.6984\n",
      "Epoch 46/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.0730 - accuracy: 0.9783 - val_loss: 2.0152 - val_accuracy: 0.7016\n",
      "Epoch 47/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.0691 - accuracy: 0.9785 - val_loss: 2.0710 - val_accuracy: 0.6932\n",
      "Epoch 48/70\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0806 - accuracy: 0.9733 - val_loss: 2.0862 - val_accuracy: 0.6936\n",
      "Epoch 49/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.0785 - accuracy: 0.9739 - val_loss: 2.0968 - val_accuracy: 0.6980\n",
      "Epoch 50/70\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0652 - accuracy: 0.9798 - val_loss: 2.1438 - val_accuracy: 0.6966\n",
      "Epoch 51/70\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0621 - accuracy: 0.9805 - val_loss: 2.1750 - val_accuracy: 0.6965\n",
      "Epoch 52/70\n",
      "391/391 [==============================] - 3s 6ms/step - loss: 0.0536 - accuracy: 0.9844 - val_loss: 2.2355 - val_accuracy: 0.6998\n",
      "Epoch 53/70\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.0618 - accuracy: 0.9807 - val_loss: 2.2956 - val_accuracy: 0.6941\n",
      "Epoch 54/70\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.0578 - accuracy: 0.9813 - val_loss: 2.3191 - val_accuracy: 0.6968\n",
      "Epoch 55/70\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0606 - accuracy: 0.9803 - val_loss: 2.3135 - val_accuracy: 0.6977\n",
      "Epoch 56/70\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0550 - accuracy: 0.9823 - val_loss: 2.3823 - val_accuracy: 0.6939\n",
      "Epoch 57/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.0531 - accuracy: 0.9831 - val_loss: 2.3867 - val_accuracy: 0.6989\n",
      "Epoch 58/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 2s 5ms/step - loss: 0.0442 - accuracy: 0.9865 - val_loss: 2.4412 - val_accuracy: 0.6928\n",
      "Epoch 59/70\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0508 - accuracy: 0.9834 - val_loss: 2.4531 - val_accuracy: 0.6948\n",
      "Epoch 60/70\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.0557 - accuracy: 0.9817 - val_loss: 2.5168 - val_accuracy: 0.6926\n",
      "Epoch 61/70\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.0486 - accuracy: 0.9844 - val_loss: 2.5106 - val_accuracy: 0.6961\n",
      "Epoch 62/70\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0416 - accuracy: 0.9869 - val_loss: 2.6373 - val_accuracy: 0.6928\n",
      "Epoch 63/70\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.0537 - accuracy: 0.9811 - val_loss: 2.6298 - val_accuracy: 0.6915\n",
      "Epoch 64/70\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0532 - accuracy: 0.9809 - val_loss: 2.6769 - val_accuracy: 0.6945\n",
      "Epoch 65/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.0364 - accuracy: 0.9890 - val_loss: 2.7066 - val_accuracy: 0.6936\n",
      "Epoch 66/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.0515 - accuracy: 0.9829 - val_loss: 2.6719 - val_accuracy: 0.6931\n",
      "Epoch 67/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.0541 - accuracy: 0.9812 - val_loss: 2.7427 - val_accuracy: 0.6953\n",
      "Epoch 68/70\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.0326 - accuracy: 0.9897 - val_loss: 2.7736 - val_accuracy: 0.6988\n",
      "Epoch 69/70\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.0191 - accuracy: 0.9958 - val_loss: 2.8017 - val_accuracy: 0.6929\n",
      "Epoch 70/70\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.0442 - accuracy: 0.9850 - val_loss: 2.8566 - val_accuracy: 0.6876\n"
     ]
    }
   ],
   "source": [
    "cnn = models.Sequential([\n",
    "    layers.Conv2D(filters=64, kernel_size=3, activation='relu', \n",
    "                  kernel_initializer='he_uniform', padding='same', \n",
    "                  input_shape=image_shape),\n",
    "    layers.MaxPooling2D(pool_size=2, strides=2),\n",
    "    layers.Conv2D(filters=64, kernel_size=3, activation='relu', \n",
    "                  kernel_initializer='he_uniform', padding='same'),\n",
    "    layers.MaxPooling2D(pool_size=2, strides=2),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(10, activation='softmax')],\n",
    "    name='cnn')\n",
    "\n",
    "train_model(cnn, optimizer=optimizers.Adam, learning_rate=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ec21497c60752bff",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Does it improve the result? Does it run faster than the MLP?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-e6010db8bc2020df",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "The accuracy improves significantly, but the training time increases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2ee4f12368d36c69",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "How many parameters does this model have? How many parameters has the MLP? Show the steps of your computation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-599a5093a914d24d",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "The number of parameters in the weight of a 2D convolutional filter is $\\text{input_channels} \\cdot \\text{output_channels} \\cdot \\text{kernel_size}^2$. The bias has a parameter for each output channel. Thus the first convolution has $1728+64$ parameters, the second has $36864+64$ parameters. After applying pooling twice, the feature maps have size $8\\times8$ with $64$ channels, after flattening this corresponds to $4096$ features. Thus, the final dense layer has $4096\\cdot10+10=40970$ parameters. Summing everything together gives $79690$ parameters.\n",
    "\n",
    "The MLP receives $32\\cdot32\\cdot3=3072$ features as input, thus the total number of parameters is $3072\\cdot512+512+512\\cdot10+10=1578506$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ae5c926bd88fcedb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Implementing the Forward Pass\n",
    "\n",
    "To confirm your understanding of the main CNN components, implement the forward pass of the convolutional, max pooling and dense layers, plus the relu and softmax activation functions. For simplicity, assume a fixed filter size of 3x3 for the convolution, with stride 1 and use zero padding, such that the spatial size does not change (called 'same' padding). Implement this in `conv3x3_same`. For max pooling assume a fixed 2x2 pooling size and stride 2 in `maxpool2x2`.\n",
    "\n",
    "To check whether your implementation is correct, we can extract the weights from the Keras model we trained above, and feed these weights and an test input to your implementation of the forward pass. If your result disagrees with Keras, there is probably a bug somewhere!\n",
    "\n",
    "You can also generalize these to other filter sizes and strides as well.\n",
    "(Implementation of the backward pass does not fit within this exercise, but the \"Machine Learning\" course of our chair does include such exercises.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-e8926ce2153b13ca",
     "locked": false,
     "points": 7,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Congratulations, you got correct results!\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def conv3x3_same(x, weights, biases):\n",
    "    \"\"\"Convolutional layer with filter size 3x3 and 'same' padding.\n",
    "    `x` is a NumPy array of shape [height, width, n_features_in]\n",
    "    `weights` has shape [3, 3, n_features_in, n_features_out]\n",
    "    `biases` has shape [n_features_out]\n",
    "    Return the output of the 3x3 conv (without activation)\n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION\n",
    "    x = np.pad(x, [(1, 1), (1, 1), (0, 0)], mode='constant')\n",
    "    result = np.empty((x.shape[0], x.shape[1], weights.shape[3]))\n",
    "    for i in range(1, x.shape[0] - 1):\n",
    "        for j in range(1, x.shape[1] - 1):\n",
    "            roi = x[i-1:i+2, j-1:j+2]\n",
    "            for c_out in range(weights.shape[3]):\n",
    "                result[i, j, c_out] = np.sum(roi * weights[..., c_out])\n",
    "\n",
    "    result = result[1:-1, 1:-1]\n",
    "    result += biases\n",
    "    ### END SOLUTION\n",
    "    return result\n",
    "\n",
    "def maxpool2x2(x):\n",
    "    \"\"\"Max pooling with pool size 2x2 and stride 2.\n",
    "    `x` is a numpy array of shape [height, width, n_features]\n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION\n",
    "    result = np.empty((x.shape[0]//2, x.shape[1]//2, x.shape[2]))\n",
    "    for i in range(0, result.shape[0]):\n",
    "        for j in range(0, result.shape[1]):\n",
    "            roi = x[i*2:i*2+2, j*2:j*2+2]\n",
    "            result[i, j] = np.max(roi, axis=(0,1))\n",
    "    ### END SOLUTION\n",
    "    return result\n",
    "\n",
    "def dense(x, weights, biases):\n",
    "    ### BEGIN SOLUTION\n",
    "    return x.T @ weights + biases\n",
    "    ### END SOLUTION\n",
    "    \n",
    "def relu(x):\n",
    "    ### BEGIN SOLUTION\n",
    "    return np.maximum(0, x)\n",
    "    ### END SOLUTION\n",
    "\n",
    "def softmax(x):\n",
    "    ### BEGIN SOLUTION\n",
    "    maxi = np.max(x)\n",
    "    exponentiated = np.exp(x-maxi)\n",
    "    return exponentiated / np.sum(exponentiated)\n",
    "    ### END SOLUTION\n",
    "\n",
    "def my_predict_cnn(x, W1, b1, W2, b2, W3, b3):\n",
    "    x = conv3x3_same(x, W1, b1)\n",
    "    x = relu(x)\n",
    "    x = maxpool2x2(x)\n",
    "    x = conv3x3_same(x, W2, b2)\n",
    "    x = relu(x)\n",
    "    x = maxpool2x2(x)\n",
    "    x = x.reshape(-1)\n",
    "    x = dense(x, W3, b3)\n",
    "    x = softmax(x)\n",
    "    return x\n",
    "\n",
    "W1, b1 = cnn.layers[0].get_weights()\n",
    "W2, b2 = cnn.layers[2].get_weights()\n",
    "W3, b3 = cnn.layers[5].get_weights()\n",
    "\n",
    "i_test = 12\n",
    "inp = x_test[i_test]\n",
    "my_prob = my_predict_cnn(inp, W1, b1, W2, b2, W3, b3)\n",
    "keras_prob = cnn.predict(inp[np.newaxis])[0]\n",
    "if np.mean((my_prob-keras_prob)**2) > 1e-10:\n",
    "    print('Something isn\\'t right! Keras gives different results than my_predict_cnn!')\n",
    "else:\n",
    "    print('Congratulations, you got correct results!')\n",
    "    \n",
    "i_maxpred = np.argmax(my_prob)\n",
    "plot_multiple([im_test[i_test]], [f'Pred: {labels[i_maxpred]}, {my_prob[i_maxpred]:.1%}'], imheight=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-959a911a2a175121",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Batch Normalization\n",
    "\n",
    "Batch normalization is a modern technique to improve and speed up the training of deep neural networks (BatchNorm, Ioffe & Szegedy ICML'15, https://arxiv.org/abs/1502.03167). Each feature channel is normalized to have zero mean and unit variance across the spatial and mini-batch axes. To compensate for the lost degrees of freedom, extra scaling and bias parameters are introduced and learned. Mathematically, BatchNorm for a spatial feature map (e.g. the output of conv) can be written as:\n",
    "\n",
    "$$\n",
    "\\mu_d = \\mathbb{E}\\{x_{\\cdot \\cdot d}\\}, \\\\\n",
    "\\sigma_d = \\sqrt{\\operatorname{Var}\\{x_{\\cdot \\cdot d}\\}} \\\\\n",
    "z_{ijd} = \\gamma_d \\cdot \\frac{x_{ijd} - \\mu_d}{\\sigma_d} + \\beta_d,\\\\\n",
    "$$\n",
    "\n",
    "with the expectation and variance taken across both the data samples of the batch and the spatial dimensions.\n",
    "\n",
    "The $\\mu_d$ and $\\sigma_d$ values are computed on the actual mini-batch during training, but at test-time they are fixed, so that the prediction of the final system on a given sample does not depend on other samples in the mini-batch. To obtain the fixed values for test-time use, one needs to maintain moving statistics over the activations during training. This can be a bit tricky to implement from scratch, but luckily this is now implemented in all popular frameworks, including TensorFlow and Keras.\n",
    "\n",
    "When applying BatchNorm, it is not necessary to use biases in the previous convolutional layer. Why? Use the \"use_bias\" argument of `layers.Conv2D` accordingly.\n",
    "\n",
    "Furthermore, if the BatchNorm is followed by a linear or conv layer (with perhaps a ReLU in between), it is not necessary to use the $\\gamma_d$ factor in BatchNorm (it can be turned off as `layers.BatchNormalization(scale=False)`). Why? What about $\\beta_d$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-0178a85974e9358e",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "The bias from the previous layer would be removed by the normalization.\n",
    "\n",
    "The following layer can perform the scaling, a composition of linear operations is still a linear operation. ReLU does not change this, as negative values stay negative even after scaling, if $\\gamma_d$ is positive. A negative $\\gamma_d$ would flip the sign, but that can be performed by the previous layer already. The bias term can not be omitted since it might negate selected values only."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1a1a3ac922ed7e52",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Create a modified version of the previous model, where the `Conv2D` layers don't include the activation any more, and instead, insert a `layers.BatchNormalization()` and a `layers.Activation('relu')` layer after each conv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-83b754b10f9a5f09",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/70\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 1.4204 - accuracy: 0.5032 - val_loss: 2.1012 - val_accuracy: 0.2870\n",
      "Epoch 2/70\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.0077 - accuracy: 0.6489 - val_loss: 1.0688 - val_accuracy: 0.6300\n",
      "Epoch 3/70\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.9001 - accuracy: 0.6886 - val_loss: 0.9705 - val_accuracy: 0.6663\n",
      "Epoch 4/70\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.8222 - accuracy: 0.7178 - val_loss: 0.9420 - val_accuracy: 0.6891\n",
      "Epoch 5/70\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.7531 - accuracy: 0.7400 - val_loss: 0.9257 - val_accuracy: 0.6938\n",
      "Epoch 6/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.7085 - accuracy: 0.7559 - val_loss: 0.9010 - val_accuracy: 0.7033\n",
      "Epoch 7/70\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.6577 - accuracy: 0.7738 - val_loss: 0.8968 - val_accuracy: 0.6984\n",
      "Epoch 8/70\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.6209 - accuracy: 0.7843 - val_loss: 0.8545 - val_accuracy: 0.7166\n",
      "Epoch 9/70\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.5846 - accuracy: 0.7974 - val_loss: 0.8384 - val_accuracy: 0.7266\n",
      "Epoch 10/70\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.5514 - accuracy: 0.8102 - val_loss: 0.9010 - val_accuracy: 0.7063\n",
      "Epoch 11/70\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.5228 - accuracy: 0.8186 - val_loss: 0.8530 - val_accuracy: 0.7226\n",
      "Epoch 12/70\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.4927 - accuracy: 0.8298 - val_loss: 0.8702 - val_accuracy: 0.7240\n",
      "Epoch 13/70\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.4639 - accuracy: 0.8395 - val_loss: 1.0166 - val_accuracy: 0.6870\n",
      "Epoch 14/70\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.4385 - accuracy: 0.8499 - val_loss: 0.9878 - val_accuracy: 0.7027\n",
      "Epoch 15/70\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.4205 - accuracy: 0.8530 - val_loss: 0.8743 - val_accuracy: 0.7255\n",
      "Epoch 16/70\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.3968 - accuracy: 0.8618 - val_loss: 0.8835 - val_accuracy: 0.7259\n",
      "Epoch 17/70\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.3764 - accuracy: 0.8688 - val_loss: 0.9079 - val_accuracy: 0.7251\n",
      "Epoch 18/70\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.3545 - accuracy: 0.8783 - val_loss: 0.9445 - val_accuracy: 0.7191\n",
      "Epoch 19/70\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.3409 - accuracy: 0.8815 - val_loss: 0.9008 - val_accuracy: 0.7280\n",
      "Epoch 20/70\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.3193 - accuracy: 0.8908 - val_loss: 0.9478 - val_accuracy: 0.7281\n",
      "Epoch 21/70\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.3061 - accuracy: 0.8938 - val_loss: 0.9409 - val_accuracy: 0.7308\n",
      "Epoch 22/70\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.2848 - accuracy: 0.9042 - val_loss: 0.9920 - val_accuracy: 0.7185\n",
      "Epoch 23/70\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.2731 - accuracy: 0.9074 - val_loss: 0.9941 - val_accuracy: 0.7158\n",
      "Epoch 24/70\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.2574 - accuracy: 0.9118 - val_loss: 1.0060 - val_accuracy: 0.7234\n",
      "Epoch 25/70\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.2490 - accuracy: 0.9141 - val_loss: 1.0400 - val_accuracy: 0.7214\n",
      "Epoch 26/70\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.2326 - accuracy: 0.9210 - val_loss: 1.0743 - val_accuracy: 0.7123\n",
      "Epoch 27/70\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.2236 - accuracy: 0.9238 - val_loss: 1.0224 - val_accuracy: 0.7328\n",
      "Epoch 28/70\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.2071 - accuracy: 0.9312 - val_loss: 1.0624 - val_accuracy: 0.7215\n",
      "Epoch 29/70\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.1963 - accuracy: 0.9340 - val_loss: 1.1072 - val_accuracy: 0.7196\n",
      "Epoch 30/70\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.1924 - accuracy: 0.9348 - val_loss: 1.1274 - val_accuracy: 0.7187\n",
      "Epoch 31/70\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.1823 - accuracy: 0.9393 - val_loss: 1.1154 - val_accuracy: 0.7220\n",
      "Epoch 32/70\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.1736 - accuracy: 0.9430 - val_loss: 1.1551 - val_accuracy: 0.7209\n",
      "Epoch 33/70\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.1642 - accuracy: 0.9456 - val_loss: 1.1541 - val_accuracy: 0.7203\n",
      "Epoch 34/70\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.1546 - accuracy: 0.9491 - val_loss: 1.1545 - val_accuracy: 0.7182\n",
      "Epoch 35/70\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.1492 - accuracy: 0.9520 - val_loss: 1.1781 - val_accuracy: 0.7173\n",
      "Epoch 36/70\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.1425 - accuracy: 0.9538 - val_loss: 1.2247 - val_accuracy: 0.7178\n",
      "Epoch 37/70\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.1375 - accuracy: 0.9553 - val_loss: 1.2330 - val_accuracy: 0.7146\n",
      "Epoch 38/70\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.1390 - accuracy: 0.9540 - val_loss: 1.2367 - val_accuracy: 0.7181\n",
      "Epoch 39/70\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.1260 - accuracy: 0.9606 - val_loss: 1.2677 - val_accuracy: 0.7213\n",
      "Epoch 40/70\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.1170 - accuracy: 0.9643 - val_loss: 1.2565 - val_accuracy: 0.7186\n",
      "Epoch 41/70\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.1165 - accuracy: 0.9634 - val_loss: 1.3179 - val_accuracy: 0.7132\n",
      "Epoch 42/70\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.1069 - accuracy: 0.9679 - val_loss: 1.3961 - val_accuracy: 0.7120\n",
      "Epoch 43/70\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.0967 - accuracy: 0.9725 - val_loss: 1.4267 - val_accuracy: 0.7087\n",
      "Epoch 44/70\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.1007 - accuracy: 0.9699 - val_loss: 1.4902 - val_accuracy: 0.7020\n",
      "Epoch 45/70\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.1067 - accuracy: 0.9661 - val_loss: 1.4062 - val_accuracy: 0.7107\n",
      "Epoch 46/70\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0919 - accuracy: 0.9727 - val_loss: 1.3796 - val_accuracy: 0.7200\n",
      "Epoch 47/70\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.0861 - accuracy: 0.9757 - val_loss: 1.4286 - val_accuracy: 0.7129\n",
      "Epoch 48/70\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.0830 - accuracy: 0.9769 - val_loss: 1.4329 - val_accuracy: 0.7106\n",
      "Epoch 49/70\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.0779 - accuracy: 0.9780 - val_loss: 1.4620 - val_accuracy: 0.7136\n",
      "Epoch 50/70\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0757 - accuracy: 0.9785 - val_loss: 1.5585 - val_accuracy: 0.7097\n",
      "Epoch 51/70\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0864 - accuracy: 0.9733 - val_loss: 1.4912 - val_accuracy: 0.7112\n",
      "Epoch 52/70\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.0832 - accuracy: 0.9745 - val_loss: 1.4842 - val_accuracy: 0.7150\n",
      "Epoch 53/70\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.0684 - accuracy: 0.9813 - val_loss: 1.5239 - val_accuracy: 0.7103\n",
      "Epoch 54/70\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.0712 - accuracy: 0.9793 - val_loss: 1.6837 - val_accuracy: 0.6963\n",
      "Epoch 55/70\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.0763 - accuracy: 0.9768 - val_loss: 1.6550 - val_accuracy: 0.7029\n",
      "Epoch 56/70\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.0620 - accuracy: 0.9832 - val_loss: 1.5787 - val_accuracy: 0.7101\n",
      "Epoch 57/70\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0598 - accuracy: 0.9840 - val_loss: 1.8072 - val_accuracy: 0.6903\n",
      "Epoch 58/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 3s 7ms/step - loss: 0.0645 - accuracy: 0.9818 - val_loss: 1.6613 - val_accuracy: 0.7061\n",
      "Epoch 59/70\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0710 - accuracy: 0.9784 - val_loss: 1.6182 - val_accuracy: 0.7138\n",
      "Epoch 60/70\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0516 - accuracy: 0.9871 - val_loss: 1.7255 - val_accuracy: 0.7052\n",
      "Epoch 61/70\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.0486 - accuracy: 0.9885 - val_loss: 1.6626 - val_accuracy: 0.7080\n",
      "Epoch 62/70\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0617 - accuracy: 0.9818 - val_loss: 1.6305 - val_accuracy: 0.7164\n",
      "Epoch 63/70\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0618 - accuracy: 0.9819 - val_loss: 1.6023 - val_accuracy: 0.7161\n",
      "Epoch 64/70\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.0622 - accuracy: 0.9816 - val_loss: 1.6637 - val_accuracy: 0.7118\n",
      "Epoch 65/70\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.0467 - accuracy: 0.9880 - val_loss: 1.6758 - val_accuracy: 0.7109\n",
      "Epoch 66/70\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.0394 - accuracy: 0.9909 - val_loss: 1.7048 - val_accuracy: 0.7112\n",
      "Epoch 67/70\n",
      "391/391 [==============================] - 3s 6ms/step - loss: 0.0475 - accuracy: 0.9873 - val_loss: 1.8093 - val_accuracy: 0.7008\n",
      "Epoch 68/70\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0606 - accuracy: 0.9805 - val_loss: 1.7185 - val_accuracy: 0.7171\n",
      "Epoch 69/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.0452 - accuracy: 0.9878 - val_loss: 1.6970 - val_accuracy: 0.7123\n",
      "Epoch 70/70\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.0405 - accuracy: 0.9899 - val_loss: 1.7885 - val_accuracy: 0.7054\n"
     ]
    }
   ],
   "source": [
    "cnn_batchnorm = models.Sequential([\n",
    "    # ....\n",
    "])\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "cnn_batchnorm = models.Sequential([\n",
    "    layers.Conv2D(64, (3, 3), use_bias=False,\n",
    "                  padding='same', input_shape=image_shape),\n",
    "    layers.BatchNormalization(scale=False),\n",
    "    layers.Activation('relu'),\n",
    "    layers.MaxPooling2D((2, 2), (2,2)),\n",
    "    layers.Conv2D(64, (3, 3), use_bias=False,\n",
    "                  padding='same'),\n",
    "    layers.BatchNormalization(scale=False),\n",
    "    layers.Activation('relu'),\n",
    "    layers.MaxPooling2D((2, 2), (2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(10, activation='softmax')],\n",
    "    name='cnn_batchnorm')\n",
    "### END SOLUTION\n",
    "\n",
    "train_model(cnn_batchnorm, optimizer=optimizers.Adam, learning_rate=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-57707594f666ee77",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Strided Convolutions\n",
    "\n",
    "Max-pooling is a popular technique for reducing the spatial dimensionality\n",
    "of the outputs from conv layers. Another way to reduce dimensionality is striding. For an argument why this may be similarly effective, see [Springenberg et al., ICLRW'15](https://arxiv.org/pdf/1412.6806.pdf).\n",
    "\n",
    "Now create a model using the same architecture as before, with the difference of\n",
    "removing the max-pooling layers and increasing the stride parameter of the conv layers to $2 \\times 2$ in the spatial dimensions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-34f5d6a1166b46fa",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/70\n",
      "391/391 [==============================] - 25s 64ms/step - loss: 1.4239 - accuracy: 0.4955 - val_loss: 2.1613 - val_accuracy: 0.3090\n",
      "Epoch 2/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 1.0690 - accuracy: 0.6243 - val_loss: 1.1113 - val_accuracy: 0.6103\n",
      "Epoch 3/70\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 0.9284 - accuracy: 0.6740 - val_loss: 1.0198 - val_accuracy: 0.6505\n",
      "Epoch 4/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.8367 - accuracy: 0.7111 - val_loss: 0.9474 - val_accuracy: 0.6700\n",
      "Epoch 5/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.7662 - accuracy: 0.7331 - val_loss: 0.9883 - val_accuracy: 0.6647\n",
      "Epoch 6/70\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.7165 - accuracy: 0.7512 - val_loss: 0.9542 - val_accuracy: 0.6713\n",
      "Epoch 7/70\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.6695 - accuracy: 0.7670 - val_loss: 0.9842 - val_accuracy: 0.6726\n",
      "Epoch 8/70\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.6302 - accuracy: 0.7823 - val_loss: 0.9545 - val_accuracy: 0.6858\n",
      "Epoch 9/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.5878 - accuracy: 0.7987 - val_loss: 0.9679 - val_accuracy: 0.6826\n",
      "Epoch 10/70\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.5581 - accuracy: 0.8052 - val_loss: 0.9788 - val_accuracy: 0.6807\n",
      "Epoch 11/70\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.5200 - accuracy: 0.8208 - val_loss: 0.9910 - val_accuracy: 0.6799\n",
      "Epoch 12/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.4895 - accuracy: 0.8318 - val_loss: 0.9802 - val_accuracy: 0.6877\n",
      "Epoch 13/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.4626 - accuracy: 0.8426 - val_loss: 1.0604 - val_accuracy: 0.6730\n",
      "Epoch 14/70\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 0.4416 - accuracy: 0.8469 - val_loss: 1.0670 - val_accuracy: 0.6806\n",
      "Epoch 15/70\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.4155 - accuracy: 0.8580 - val_loss: 1.0758 - val_accuracy: 0.6753\n",
      "Epoch 16/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.3935 - accuracy: 0.8660 - val_loss: 1.0761 - val_accuracy: 0.6759\n",
      "Epoch 17/70\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.3712 - accuracy: 0.8748 - val_loss: 1.1235 - val_accuracy: 0.6708\n",
      "Epoch 18/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.3534 - accuracy: 0.8805 - val_loss: 1.0967 - val_accuracy: 0.6783\n",
      "Epoch 19/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.3357 - accuracy: 0.8868 - val_loss: 1.1881 - val_accuracy: 0.6715\n",
      "Epoch 20/70\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.3193 - accuracy: 0.8932 - val_loss: 1.2199 - val_accuracy: 0.6609\n",
      "Epoch 21/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.3007 - accuracy: 0.8991 - val_loss: 1.1944 - val_accuracy: 0.6690\n",
      "Epoch 22/70\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 0.2871 - accuracy: 0.9041 - val_loss: 1.2332 - val_accuracy: 0.6697\n",
      "Epoch 23/70\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.2678 - accuracy: 0.9130 - val_loss: 1.2414 - val_accuracy: 0.6703\n",
      "Epoch 24/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.2559 - accuracy: 0.9162 - val_loss: 1.2869 - val_accuracy: 0.6679\n",
      "Epoch 25/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.2409 - accuracy: 0.9216 - val_loss: 1.2934 - val_accuracy: 0.6658\n",
      "Epoch 26/70\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.2319 - accuracy: 0.9260 - val_loss: 1.3479 - val_accuracy: 0.6615\n",
      "Epoch 27/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.2139 - accuracy: 0.9315 - val_loss: 1.3670 - val_accuracy: 0.6677\n",
      "Epoch 28/70\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.2127 - accuracy: 0.9312 - val_loss: 1.3821 - val_accuracy: 0.6658\n",
      "Epoch 29/70\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.1985 - accuracy: 0.9367 - val_loss: 1.4317 - val_accuracy: 0.6608\n",
      "Epoch 30/70\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 0.1853 - accuracy: 0.9425 - val_loss: 1.4530 - val_accuracy: 0.6630\n",
      "Epoch 31/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.1799 - accuracy: 0.9440 - val_loss: 1.4982 - val_accuracy: 0.6587\n",
      "Epoch 32/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.1681 - accuracy: 0.9488 - val_loss: 1.5376 - val_accuracy: 0.6566\n",
      "Epoch 33/70\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.1645 - accuracy: 0.9491 - val_loss: 1.4966 - val_accuracy: 0.6648\n",
      "Epoch 34/70\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.1597 - accuracy: 0.9505 - val_loss: 1.5967 - val_accuracy: 0.6583\n",
      "Epoch 35/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.1474 - accuracy: 0.9553 - val_loss: 1.5475 - val_accuracy: 0.6689\n",
      "Epoch 36/70\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.1413 - accuracy: 0.9574 - val_loss: 1.5863 - val_accuracy: 0.6593\n",
      "Epoch 37/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.1385 - accuracy: 0.9575 - val_loss: 1.6062 - val_accuracy: 0.6634\n",
      "Epoch 38/70\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.1269 - accuracy: 0.9634 - val_loss: 1.6393 - val_accuracy: 0.6590\n",
      "Epoch 39/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.1204 - accuracy: 0.9653 - val_loss: 1.6989 - val_accuracy: 0.6506\n",
      "Epoch 40/70\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 0.1211 - accuracy: 0.9639 - val_loss: 1.6722 - val_accuracy: 0.6587\n",
      "Epoch 41/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.1130 - accuracy: 0.9677 - val_loss: 1.7275 - val_accuracy: 0.6556\n",
      "Epoch 42/70\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.1082 - accuracy: 0.9693 - val_loss: 1.7217 - val_accuracy: 0.6632\n",
      "Epoch 43/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.1020 - accuracy: 0.9717 - val_loss: 1.7884 - val_accuracy: 0.6519\n",
      "Epoch 44/70\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.1009 - accuracy: 0.9715 - val_loss: 1.7969 - val_accuracy: 0.6564\n",
      "Epoch 45/70\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0958 - accuracy: 0.9741 - val_loss: 1.8151 - val_accuracy: 0.6556\n",
      "Epoch 46/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.0868 - accuracy: 0.9771 - val_loss: 1.9236 - val_accuracy: 0.6451\n",
      "Epoch 47/70\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0924 - accuracy: 0.9733 - val_loss: 1.8739 - val_accuracy: 0.6587\n",
      "Epoch 48/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.0912 - accuracy: 0.9740 - val_loss: 1.8683 - val_accuracy: 0.6611\n",
      "Epoch 49/70\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 0.0836 - accuracy: 0.9778 - val_loss: 1.9540 - val_accuracy: 0.6513\n",
      "Epoch 50/70\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0769 - accuracy: 0.9800 - val_loss: 1.9161 - val_accuracy: 0.6574\n",
      "Epoch 51/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.0803 - accuracy: 0.9776 - val_loss: 1.9470 - val_accuracy: 0.6542\n",
      "Epoch 52/70\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0809 - accuracy: 0.9773 - val_loss: 1.9584 - val_accuracy: 0.6552\n",
      "Epoch 53/70\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0707 - accuracy: 0.9813 - val_loss: 1.9989 - val_accuracy: 0.6464\n",
      "Epoch 54/70\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0674 - accuracy: 0.9834 - val_loss: 2.0196 - val_accuracy: 0.6535\n",
      "Epoch 55/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.0662 - accuracy: 0.9828 - val_loss: 2.0243 - val_accuracy: 0.6515\n",
      "Epoch 56/70\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0653 - accuracy: 0.9828 - val_loss: 2.0456 - val_accuracy: 0.6564\n",
      "Epoch 57/70\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 0.0706 - accuracy: 0.9804 - val_loss: 2.0590 - val_accuracy: 0.6559\n",
      "Epoch 58/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 2s 5ms/step - loss: 0.0663 - accuracy: 0.9825 - val_loss: 2.0898 - val_accuracy: 0.6567\n",
      "Epoch 59/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.0626 - accuracy: 0.9834 - val_loss: 2.1223 - val_accuracy: 0.6521\n",
      "Epoch 60/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.0559 - accuracy: 0.9866 - val_loss: 2.1103 - val_accuracy: 0.6596\n",
      "Epoch 61/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.0516 - accuracy: 0.9877 - val_loss: 2.2021 - val_accuracy: 0.6492\n",
      "Epoch 62/70\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 0.0564 - accuracy: 0.9855 - val_loss: 2.2163 - val_accuracy: 0.6532\n",
      "Epoch 63/70\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0581 - accuracy: 0.9846 - val_loss: 2.1619 - val_accuracy: 0.6552\n",
      "Epoch 64/70\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0568 - accuracy: 0.9852 - val_loss: 2.2672 - val_accuracy: 0.6475\n",
      "Epoch 65/70\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0504 - accuracy: 0.9882 - val_loss: 2.2463 - val_accuracy: 0.6512\n",
      "Epoch 66/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.0517 - accuracy: 0.9863 - val_loss: 2.2270 - val_accuracy: 0.6581\n",
      "Epoch 67/70\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 0.0476 - accuracy: 0.9882 - val_loss: 2.3169 - val_accuracy: 0.6458\n",
      "Epoch 68/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.0497 - accuracy: 0.9872 - val_loss: 2.2657 - val_accuracy: 0.6517\n",
      "Epoch 69/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.0513 - accuracy: 0.9870 - val_loss: 2.3309 - val_accuracy: 0.6531\n",
      "Epoch 70/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.0596 - accuracy: 0.9823 - val_loss: 2.3092 - val_accuracy: 0.6542\n"
     ]
    }
   ],
   "source": [
    "cnn_strides = models.Sequential([\n",
    "    # ....\n",
    "])\n",
    "### BEGIN SOLUTION\n",
    "cnn_strides = models.Sequential([\n",
    "    layers.Conv2D(64, 3, strides=2, use_bias=False, \n",
    "                  padding='same', input_shape=image_shape),\n",
    "    layers.BatchNormalization(scale=False),\n",
    "    layers.Activation('relu'),\n",
    "    layers.Conv2D(64, 3, strides=2, use_bias=False, padding='same'),\n",
    "    layers.BatchNormalization(scale=False),\n",
    "    layers.Activation('relu'),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(10, activation='softmax')],\n",
    "    name='cnn_strides')\n",
    "### END SOLUTION\n",
    "\n",
    "train_model(cnn_strides, optimizer=optimizers.Adam, learning_rate=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-cbb6e2eeb2dcc66e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "What differences do you notice when training this new network?\n",
    "What is a clear advantage of using strides?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-12ee310e8234b8f8",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "Training is faster. For pooling with size 2 all four features have to be computed to get the max/mean of each window. For striding only a single feature per window has to be computed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-bc32808a463893fd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Global Pooling\n",
    "\n",
    "The above network ends in a `Flatten` layer followed by a `Dense` layer, in which the number of weights depends on the input size. This means that testing can only be performed on the exact same image size. Several architectures employ a (spatial) **global average pooling layer** to produce of vector of fixed size describing the whole image, instead of flattening.\n",
    "\n",
    "For this to work well, the units before the average pooling need to have a large enough receptive field. Therefore, compared with the previous model, remove the `Flatten` layer and instead add a third Conv-BatchNorm-ReLU combination, followed by a `layers.GlobalAveragePooling2D()` layer (before the final `Dense` layer).\n",
    "\n",
    "Train it and see if it reaches similar accuracy to the previous one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-384e1eaafbd3f3b6",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/70\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 1.5492 - accuracy: 0.4496 - val_loss: 2.9942 - val_accuracy: 0.1452\n",
      "Epoch 2/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 1.2510 - accuracy: 0.5577 - val_loss: 1.2729 - val_accuracy: 0.5372\n",
      "Epoch 3/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 1.1387 - accuracy: 0.5975 - val_loss: 1.1672 - val_accuracy: 0.5846\n",
      "Epoch 4/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 1.0563 - accuracy: 0.6286 - val_loss: 1.0809 - val_accuracy: 0.6154\n",
      "Epoch 5/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 1.0044 - accuracy: 0.6445 - val_loss: 1.1021 - val_accuracy: 0.6102\n",
      "Epoch 6/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.9620 - accuracy: 0.6604 - val_loss: 1.1953 - val_accuracy: 0.5747\n",
      "Epoch 7/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.9278 - accuracy: 0.6742 - val_loss: 1.0508 - val_accuracy: 0.6261\n",
      "Epoch 8/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.8942 - accuracy: 0.6879 - val_loss: 1.0376 - val_accuracy: 0.6285\n",
      "Epoch 9/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.8673 - accuracy: 0.6964 - val_loss: 1.0436 - val_accuracy: 0.6309\n",
      "Epoch 10/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.8439 - accuracy: 0.7024 - val_loss: 1.0556 - val_accuracy: 0.6272\n",
      "Epoch 11/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.8194 - accuracy: 0.7148 - val_loss: 0.9744 - val_accuracy: 0.6574\n",
      "Epoch 12/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.8003 - accuracy: 0.7206 - val_loss: 1.2619 - val_accuracy: 0.5743\n",
      "Epoch 13/70\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.7838 - accuracy: 0.7247 - val_loss: 0.9523 - val_accuracy: 0.6655\n",
      "Epoch 14/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.7636 - accuracy: 0.7343 - val_loss: 1.0503 - val_accuracy: 0.6342\n",
      "Epoch 15/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.7502 - accuracy: 0.7380 - val_loss: 0.9415 - val_accuracy: 0.6699\n",
      "Epoch 16/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.7319 - accuracy: 0.7464 - val_loss: 0.9637 - val_accuracy: 0.6626\n",
      "Epoch 17/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.7174 - accuracy: 0.7515 - val_loss: 0.8891 - val_accuracy: 0.6843\n",
      "Epoch 18/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.7053 - accuracy: 0.7544 - val_loss: 0.8885 - val_accuracy: 0.6874\n",
      "Epoch 19/70\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 0.6911 - accuracy: 0.7614 - val_loss: 1.0062 - val_accuracy: 0.6488\n",
      "Epoch 20/70\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 0.6804 - accuracy: 0.7656 - val_loss: 0.9400 - val_accuracy: 0.6775\n",
      "Epoch 21/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.6690 - accuracy: 0.7668 - val_loss: 1.0599 - val_accuracy: 0.6403\n",
      "Epoch 22/70\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.6552 - accuracy: 0.7728 - val_loss: 0.9368 - val_accuracy: 0.6781\n",
      "Epoch 23/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.6481 - accuracy: 0.7775 - val_loss: 0.9476 - val_accuracy: 0.6761\n",
      "Epoch 24/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.6335 - accuracy: 0.7820 - val_loss: 0.8852 - val_accuracy: 0.7003\n",
      "Epoch 25/70\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.6253 - accuracy: 0.7852 - val_loss: 0.8937 - val_accuracy: 0.6962\n",
      "Epoch 26/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.6155 - accuracy: 0.7888 - val_loss: 0.9160 - val_accuracy: 0.6869\n",
      "Epoch 27/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.6045 - accuracy: 0.7926 - val_loss: 0.9159 - val_accuracy: 0.6809\n",
      "Epoch 28/70\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 0.5967 - accuracy: 0.7939 - val_loss: 0.9483 - val_accuracy: 0.6797\n",
      "Epoch 29/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.5872 - accuracy: 0.7971 - val_loss: 0.8714 - val_accuracy: 0.7038\n",
      "Epoch 30/70\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 0.5814 - accuracy: 0.7999 - val_loss: 0.9991 - val_accuracy: 0.6610\n",
      "Epoch 31/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.5697 - accuracy: 0.8044 - val_loss: 0.8234 - val_accuracy: 0.7182\n",
      "Epoch 32/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.5627 - accuracy: 0.8070 - val_loss: 0.8927 - val_accuracy: 0.6997\n",
      "Epoch 33/70\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.5553 - accuracy: 0.8091 - val_loss: 0.9215 - val_accuracy: 0.6839\n",
      "Epoch 34/70\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.5520 - accuracy: 0.8108 - val_loss: 0.8890 - val_accuracy: 0.7018\n",
      "Epoch 35/70\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 0.5399 - accuracy: 0.8144 - val_loss: 0.8953 - val_accuracy: 0.7059\n",
      "Epoch 36/70\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.5342 - accuracy: 0.8169 - val_loss: 0.9564 - val_accuracy: 0.6786\n",
      "Epoch 37/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.5257 - accuracy: 0.8195 - val_loss: 0.9076 - val_accuracy: 0.6954\n",
      "Epoch 38/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.5158 - accuracy: 0.8260 - val_loss: 1.0250 - val_accuracy: 0.6680\n",
      "Epoch 39/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.5104 - accuracy: 0.8247 - val_loss: 0.9618 - val_accuracy: 0.6837\n",
      "Epoch 40/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.5057 - accuracy: 0.8278 - val_loss: 0.8595 - val_accuracy: 0.7153\n",
      "Epoch 41/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.4992 - accuracy: 0.8294 - val_loss: 0.8813 - val_accuracy: 0.6995\n",
      "Epoch 42/70\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.4903 - accuracy: 0.8343 - val_loss: 0.9040 - val_accuracy: 0.7020\n",
      "Epoch 43/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.4862 - accuracy: 0.8338 - val_loss: 0.8656 - val_accuracy: 0.7072\n",
      "Epoch 44/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.4832 - accuracy: 0.8354 - val_loss: 0.9208 - val_accuracy: 0.6944\n",
      "Epoch 45/70\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 0.4780 - accuracy: 0.8372 - val_loss: 0.8908 - val_accuracy: 0.7039\n",
      "Epoch 46/70\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 0.4694 - accuracy: 0.8406 - val_loss: 0.8750 - val_accuracy: 0.7120\n",
      "Epoch 47/70\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 0.4624 - accuracy: 0.8435 - val_loss: 0.9542 - val_accuracy: 0.6885\n",
      "Epoch 48/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.4573 - accuracy: 0.8461 - val_loss: 0.8981 - val_accuracy: 0.7086\n",
      "Epoch 49/70\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.4535 - accuracy: 0.8460 - val_loss: 0.8896 - val_accuracy: 0.7091\n",
      "Epoch 50/70\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.4444 - accuracy: 0.8493 - val_loss: 0.8811 - val_accuracy: 0.7080\n",
      "Epoch 51/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.4405 - accuracy: 0.8500 - val_loss: 0.8786 - val_accuracy: 0.7101\n",
      "Epoch 52/70\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.4339 - accuracy: 0.8544 - val_loss: 0.9322 - val_accuracy: 0.7009\n",
      "Epoch 53/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.4317 - accuracy: 0.8531 - val_loss: 0.9558 - val_accuracy: 0.6926\n",
      "Epoch 54/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.4231 - accuracy: 0.8571 - val_loss: 1.0586 - val_accuracy: 0.6791\n",
      "Epoch 55/70\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 0.4192 - accuracy: 0.8579 - val_loss: 1.0440 - val_accuracy: 0.6748\n",
      "Epoch 56/70\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 0.4192 - accuracy: 0.8573 - val_loss: 0.8962 - val_accuracy: 0.7131\n",
      "Epoch 57/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.4082 - accuracy: 0.8628 - val_loss: 0.8834 - val_accuracy: 0.7126\n",
      "Epoch 58/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 2s 5ms/step - loss: 0.4072 - accuracy: 0.8622 - val_loss: 0.9813 - val_accuracy: 0.6954\n",
      "Epoch 59/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.4014 - accuracy: 0.8654 - val_loss: 0.9484 - val_accuracy: 0.6976\n",
      "Epoch 60/70\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.3998 - accuracy: 0.8649 - val_loss: 0.9470 - val_accuracy: 0.7023\n",
      "Epoch 61/70\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.3925 - accuracy: 0.8686 - val_loss: 1.0103 - val_accuracy: 0.6779\n",
      "Epoch 62/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.3887 - accuracy: 0.8684 - val_loss: 0.9361 - val_accuracy: 0.7032\n",
      "Epoch 63/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.3861 - accuracy: 0.8693 - val_loss: 0.8914 - val_accuracy: 0.7167\n",
      "Epoch 64/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.3812 - accuracy: 0.8723 - val_loss: 0.9263 - val_accuracy: 0.7043\n",
      "Epoch 65/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.3748 - accuracy: 0.8757 - val_loss: 0.8938 - val_accuracy: 0.7156\n",
      "Epoch 66/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.3710 - accuracy: 0.8767 - val_loss: 0.9735 - val_accuracy: 0.6989\n",
      "Epoch 67/70\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.3661 - accuracy: 0.8784 - val_loss: 0.9486 - val_accuracy: 0.7039\n",
      "Epoch 68/70\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.3650 - accuracy: 0.8777 - val_loss: 0.9367 - val_accuracy: 0.7089\n",
      "Epoch 69/70\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.3604 - accuracy: 0.8804 - val_loss: 1.0671 - val_accuracy: 0.6716\n",
      "Epoch 70/70\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.3533 - accuracy: 0.8827 - val_loss: 0.9706 - val_accuracy: 0.7011\n"
     ]
    }
   ],
   "source": [
    "cnn_global_pool = models.Sequential([\n",
    "    # ....\n",
    "])\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "cnn_global_pool = models.Sequential([\n",
    "    layers.Conv2D(64, 3, 2, padding='same', use_bias=False),\n",
    "    layers.BatchNormalization(scale=False),\n",
    "    layers.Activation('relu'),\n",
    "    layers.Conv2D(64, 3, 2, padding='same', use_bias=False),\n",
    "    layers.BatchNormalization(scale=False),\n",
    "    layers.Activation('relu'),\n",
    "    layers.Conv2D(64, 3, padding='same', use_bias=False),\n",
    "    layers.BatchNormalization(scale=False),\n",
    "    layers.Activation('relu'),\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dense(10, activation='softmax')],\n",
    "    name='cnn_global_pool')\n",
    "### END SOLUTION\n",
    "\n",
    "train_model(cnn_global_pool, optimizer=optimizers.Adam, learning_rate=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-527e589864d3a660",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Which network has more parameters, this or the previous one?\n",
    "\n",
    "What is the size of the receptive field of the units in the layer directly before the global average pooling? (Remember: the receptive field of a particular unit (neuron) is the area of the *input image* that can influence the activation of this given unit)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-a36f95947bd5ba86",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "The first layers are the same in both models, so we only need to compare the last layers. The previous model has $40970$ parameters in the last dense layer. The new model has a convolutional layer with $36864$ weights, followed by a bias term (in the BatchNorm) with $64$ parameters. After the global pooling, $64$ features are left, so the final dense layer has $640$ weights and a bias of size $10$. $37578$ parameters in total.\n",
    "\n",
    "The following figure shows the receptive fields of the convolutions. The first convolution has a receptive field of $3\\times3$. This is applied with stride 2, so the second convolution has a receptive field of size $7\\times7$. This is again applied with stride 2, thus the final layer has stride $15\\times15$.\n",
    "\n",
    "       conv1  conv2  conv3\n",
    "    00 \\\n",
    "    01 |-      \\\n",
    "    02 /  \\    |\n",
    "    03    |-   |-     \\\n",
    "    04 \\  /    |      |\n",
    "    05 |-      / \\    |\n",
    "    06 /  \\      |    |\n",
    "    07    |-     |-   |-\n",
    "    08 \\  /      |    |\n",
    "    09 |-      \\ /    |\n",
    "    10 /  \\    |      |\n",
    "    11    |-   |-     /\n",
    "    12 \\  /    |\n",
    "    13 |-      /\n",
    "    14 /\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-cfc670e8ea147092",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Residual Networks\n",
    "\n",
    "ResNet is a more modern architecture, introduced by He et al. in 2015 (published in 2016: https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf) and is still popular today.\n",
    "\n",
    "It consists of blocks like the following:\n",
    "\n",
    "![ResNet Block](resnet_block.png)\n",
    "\n",
    "Each of these so-called *residual blocks* only have to predict a *residual* (in plain words: the \"rest\", the \"leftover\") that will be added on top of its input.\n",
    "In other words, the block outputs how much each feature needs to be changed in order to enhance the representation compared to the previous block.\n",
    "\n",
    "There are several ways to combine residual blocks into *residual networks* (ResNets). In the following, we consider ResNet-v1, as used for the CIFAR-10 benchmark in the original ResNet paper (it is simpler compared to the full model that they used for the much larger ImageNet benchmark).\n",
    "\n",
    "Section 4.2. of the paper describes this architecture as follows: \"*The first layer is 3×3 convolutions. Then we use a stack of 6n layers with 3×3 convolutions on the feature maps of sizes {32, 16, 8} respectively, with 2n layers for each feature map size. The numbers of filters are {16, 32, 64} respectively. The subsampling is performed by convolutions with a stride of 2. The network ends with a global average pooling, a 10-way fully-connected layer, and softmax. [...] When shortcut connections are used, they are connected to the pairs of 3×3 layers (totally 3n shortcuts). On this dataset we use identity shortcuts in all cases.*\"\n",
    "\n",
    "Further, they use L2 regularization for training (a standard tool to combat overfitting). This penalizes weights with large magnitude by adding an additional term to the cost function, besides the cross-entropy. The overall function to optimize becomes:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{CE} + \\frac{\\lambda}{2} \\sum_{w\\in\\text{weights}} w^2,\n",
    "$$\n",
    "\n",
    "and in this paper $\\lambda=10^{-4}$.\n",
    "\n",
    "In the previous parts of this exercise we have already seen every major component we need to build this thing. However, ResNet is not a pure sequential architecture due to the skip connections. This means we cannot use `models.Sequential`. Luckily, Keras also offers a functional API. Look below to understand how this API works and fill in the missing pieces to make a ResNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-78512b7752e9d7d2",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def resnet(num_layers=56):\n",
    "    if (num_layers - 2) % 6 != 0:\n",
    "        raise ValueError('n_layers should be 6n+2 (eg 20, 32, 44, 56)')\n",
    "    n = (num_layers - 2) // 6\n",
    "        \n",
    "    inputs = layers.Input(shape=image_shape)\n",
    "    \n",
    "    # First layer\n",
    "    x = layers.Conv2D(16, 3, use_bias=False, \n",
    "        kernel_regularizer=regularizers.l2(1e-4),\n",
    "        padding='same', kernel_initializer='he_normal')(inputs)\n",
    "    x = layers.BatchNormalization(scale=False)(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    \n",
    "    # Call the `resnet_block` function in loops to stack ResNet blocks according to the instructions above.\n",
    "    ### BEGIN SOLUTION\n",
    "    for i_block in range(n):\n",
    "        x = resnet_block(x, 16, strides=1)\n",
    "        \n",
    "    for i_block in range(n):\n",
    "        x = resnet_block(x, 32, strides=2 if i_block==0 else 1)\n",
    "        \n",
    "    for i_block in range(n):\n",
    "        x = resnet_block(x, 64, strides=2 if i_block==0 else 1)\n",
    "    ### END SOLUTION\n",
    "\n",
    "    # Global pooling and classifier on top\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    outputs = layers.Dense(10, activation='softmax',\n",
    "            kernel_regularizer=regularizers.l2(1e-4))(x)\n",
    "    return models.Model(inputs=inputs, outputs=outputs, name=f'resnet{num_layers}')\n",
    "\n",
    "def resnet_block(x, n_channels_out, strides=1):\n",
    "    # First conv\n",
    "    f = layers.Conv2D(n_channels_out, 3, strides, use_bias=False,\n",
    "            kernel_regularizer=regularizers.l2(1e-4),\n",
    "            padding='same', kernel_initializer='he_normal')(x)\n",
    "    f = layers.BatchNormalization(scale=False)(f)\n",
    "    f = layers.Activation('relu')(f)\n",
    "\n",
    "    # Second conv\n",
    "    ### BEGIN SOLUTION\n",
    "    f = layers.Conv2D(n_channels_out, 3, use_bias=False,\n",
    "            kernel_regularizer=regularizers.l2(1e-4),\n",
    "            padding='same', kernel_initializer='he_normal')(f)\n",
    "    f = layers.BatchNormalization(scale=False)(f)\n",
    "    ### END SOLUTION\n",
    "    \n",
    "    # The shortcut connection is just the identity.\n",
    "    # If feature channel counts differ between input and output,\n",
    "    # zero padding is used to match the depths.\n",
    "    # This is implemented by a Conv2D with fixed weights.\n",
    "    n_channels_in = x.shape[-1]\n",
    "    if n_channels_in != n_channels_out:\n",
    "        # Fixed weights, np.eye returns a matrix with 1s along the \n",
    "        # main diagonal and zeros elsewhere.\n",
    "        identity_weights = np.eye(n_channels_in, n_channels_out, dtype=np.float32)\n",
    "        layer = layers.Conv2D(\n",
    "            n_channels_out, kernel_size=1, strides=strides, use_bias=False, \n",
    "            kernel_initializer=initializers.Constant(value=identity_weights))\n",
    "        # Not learned! Set trainable to False:\n",
    "        layer.trainable = False\n",
    "        x = layer(x)\n",
    "       \n",
    "    # This is where the ResNet magic happens: the shortcut connection is\n",
    "    # added to the residual.\n",
    "    x = layers.add([x, f])\n",
    "    return layers.Activation('relu')(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-434819020b48d6b4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Learning Rate Decay and Data Augmentation - Our Final Model\n",
    "\n",
    "Learning rate decay reduces the learning rate as the training progresses. It can be implemented as a Keras callback as shown below.\n",
    "\n",
    "If you have a good GPU or a lot of time, train ResNet-56 on the CIFAR-10 dataset for 75 epochs. As a rough idea, it will take about one hour with a good GPU, but on a CPU it could take a day or two. If that's too long, train a smaller ResNet, wih `num_layers`=14 or 20, or do fewer epochs.\n",
    "\n",
    "To add data augmentation (e.g. random translation or rotation of the input images), look up the documentation for the `ImageDataGenerator` class. The ResNet model presented in the original paper was trained with random translations of $\\pm$ 4 px."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-f2042420b7d15963",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/70\n",
      "  2/391 [..............................] - ETA: 1:10 - loss: 5.4224 - accuracy: 0.1094WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.164536). Check your callbacks.\n",
      "391/391 [==============================] - 89s 228ms/step - loss: 1.8500 - accuracy: 0.4760 - val_loss: 2.1800 - val_accuracy: 0.4119 - lr: 0.0010\n",
      "Epoch 2/70\n",
      "391/391 [==============================] - 14s 37ms/step - loss: 1.3559 - accuracy: 0.6540 - val_loss: 1.9082 - val_accuracy: 0.5010 - lr: 0.0010\n",
      "Epoch 3/70\n",
      "391/391 [==============================] - 14s 36ms/step - loss: 1.1318 - accuracy: 0.7265 - val_loss: 1.4208 - val_accuracy: 0.6341 - lr: 0.0010\n",
      "Epoch 4/70\n",
      "391/391 [==============================] - 14s 37ms/step - loss: 0.9790 - accuracy: 0.7763 - val_loss: 1.0791 - val_accuracy: 0.7399 - lr: 0.0010\n",
      "Epoch 5/70\n",
      "391/391 [==============================] - 15s 38ms/step - loss: 0.8739 - accuracy: 0.8072 - val_loss: 1.7142 - val_accuracy: 0.5939 - lr: 0.0010\n",
      "Epoch 6/70\n",
      "391/391 [==============================] - 14s 37ms/step - loss: 0.7902 - accuracy: 0.8337 - val_loss: 1.6002 - val_accuracy: 0.6185 - lr: 0.0010\n",
      "Epoch 7/70\n",
      "391/391 [==============================] - 15s 38ms/step - loss: 0.7196 - accuracy: 0.8562 - val_loss: 1.2158 - val_accuracy: 0.7132 - lr: 0.0010\n",
      "Epoch 8/70\n",
      "391/391 [==============================] - 15s 38ms/step - loss: 0.6650 - accuracy: 0.8727 - val_loss: 1.1636 - val_accuracy: 0.7323 - lr: 0.0010\n",
      "Epoch 9/70\n",
      "391/391 [==============================] - 15s 38ms/step - loss: 0.6197 - accuracy: 0.8863 - val_loss: 1.6861 - val_accuracy: 0.6430 - lr: 0.0010\n",
      "Epoch 10/70\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.5775 - accuracy: 0.9002 - val_loss: 1.2516 - val_accuracy: 0.7254 - lr: 0.0010\n",
      "Epoch 11/70\n",
      "391/391 [==============================] - 15s 38ms/step - loss: 0.5365 - accuracy: 0.9142 - val_loss: 1.4910 - val_accuracy: 0.6858 - lr: 0.0010\n",
      "Epoch 12/70\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.5179 - accuracy: 0.9203 - val_loss: 1.5883 - val_accuracy: 0.6754 - lr: 0.0010\n",
      "Epoch 13/70\n",
      "391/391 [==============================] - 15s 37ms/step - loss: 0.4911 - accuracy: 0.9311 - val_loss: 1.2599 - val_accuracy: 0.7534 - lr: 0.0010\n",
      "Epoch 14/70\n",
      "391/391 [==============================] - 15s 37ms/step - loss: 0.4749 - accuracy: 0.9367 - val_loss: 1.5856 - val_accuracy: 0.6987 - lr: 0.0010\n",
      "Epoch 15/70\n",
      "391/391 [==============================] - 15s 37ms/step - loss: 0.4495 - accuracy: 0.9454 - val_loss: 1.8695 - val_accuracy: 0.6623 - lr: 0.0010\n",
      "Epoch 16/70\n",
      "391/391 [==============================] - 15s 38ms/step - loss: 0.4451 - accuracy: 0.9454 - val_loss: 1.4483 - val_accuracy: 0.7282 - lr: 0.0010\n",
      "Epoch 17/70\n",
      "391/391 [==============================] - 15s 38ms/step - loss: 0.4348 - accuracy: 0.9501 - val_loss: 2.2051 - val_accuracy: 0.6385 - lr: 0.0010\n",
      "Epoch 18/70\n",
      "391/391 [==============================] - 14s 37ms/step - loss: 0.4389 - accuracy: 0.9496 - val_loss: 1.4374 - val_accuracy: 0.7439 - lr: 0.0010\n",
      "Epoch 19/70\n",
      "391/391 [==============================] - 14s 37ms/step - loss: 0.4200 - accuracy: 0.9565 - val_loss: 2.3222 - val_accuracy: 0.6489 - lr: 0.0010\n",
      "Epoch 20/70\n",
      "391/391 [==============================] - 15s 37ms/step - loss: 0.4172 - accuracy: 0.9565 - val_loss: 2.1496 - val_accuracy: 0.6448 - lr: 0.0010\n",
      "Epoch 21/70\n",
      "391/391 [==============================] - 15s 38ms/step - loss: 0.4043 - accuracy: 0.9606 - val_loss: 1.8170 - val_accuracy: 0.6801 - lr: 0.0010\n",
      "Epoch 22/70\n",
      "391/391 [==============================] - 15s 38ms/step - loss: 0.4105 - accuracy: 0.9585 - val_loss: 3.3883 - val_accuracy: 0.5541 - lr: 0.0010\n",
      "Epoch 23/70\n",
      "391/391 [==============================] - 15s 38ms/step - loss: 0.4001 - accuracy: 0.9623 - val_loss: 1.6403 - val_accuracy: 0.7175 - lr: 0.0010\n",
      "Epoch 24/70\n",
      "391/391 [==============================] - 15s 38ms/step - loss: 0.3968 - accuracy: 0.9630 - val_loss: 2.0650 - val_accuracy: 0.6414 - lr: 0.0010\n",
      "Epoch 25/70\n",
      "391/391 [==============================] - 15s 37ms/step - loss: 0.4133 - accuracy: 0.9564 - val_loss: 1.8871 - val_accuracy: 0.6808 - lr: 0.0010\n",
      "Epoch 26/70\n",
      "391/391 [==============================] - 15s 38ms/step - loss: 0.3870 - accuracy: 0.9666 - val_loss: 1.7292 - val_accuracy: 0.7112 - lr: 0.0010\n",
      "Epoch 27/70\n",
      "391/391 [==============================] - 15s 38ms/step - loss: 0.3950 - accuracy: 0.9616 - val_loss: 1.8300 - val_accuracy: 0.7003 - lr: 0.0010\n",
      "Epoch 28/70\n",
      "391/391 [==============================] - 15s 38ms/step - loss: 0.3844 - accuracy: 0.9666 - val_loss: 1.6563 - val_accuracy: 0.7163 - lr: 0.0010\n",
      "Epoch 29/70\n",
      "391/391 [==============================] - 14s 37ms/step - loss: 0.3856 - accuracy: 0.9659 - val_loss: 1.7888 - val_accuracy: 0.7023 - lr: 0.0010\n",
      "Epoch 30/70\n",
      "391/391 [==============================] - 15s 38ms/step - loss: 0.3827 - accuracy: 0.9655 - val_loss: 2.2107 - val_accuracy: 0.6601 - lr: 0.0010\n",
      "Epoch 31/70\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.3831 - accuracy: 0.9666 - val_loss: 1.6107 - val_accuracy: 0.7278 - lr: 0.0010\n",
      "Epoch 32/70\n",
      "391/391 [==============================] - 15s 38ms/step - loss: 0.3774 - accuracy: 0.9675 - val_loss: 2.2438 - val_accuracy: 0.6979 - lr: 0.0010\n",
      "Epoch 33/70\n",
      "391/391 [==============================] - 15s 37ms/step - loss: 0.3796 - accuracy: 0.9664 - val_loss: 1.4040 - val_accuracy: 0.7558 - lr: 0.0010\n",
      "Epoch 34/70\n",
      "391/391 [==============================] - 15s 38ms/step - loss: 0.3743 - accuracy: 0.9682 - val_loss: 1.8275 - val_accuracy: 0.7144 - lr: 0.0010\n",
      "Epoch 35/70\n",
      "391/391 [==============================] - 15s 37ms/step - loss: 0.3647 - accuracy: 0.9716 - val_loss: 3.5023 - val_accuracy: 0.5543 - lr: 0.0010\n",
      "Epoch 36/70\n",
      "391/391 [==============================] - 15s 38ms/step - loss: 0.3708 - accuracy: 0.9688 - val_loss: 2.0103 - val_accuracy: 0.6944 - lr: 0.0010\n",
      "Epoch 37/70\n",
      "391/391 [==============================] - 15s 38ms/step - loss: 0.3717 - accuracy: 0.9685 - val_loss: 1.4281 - val_accuracy: 0.7589 - lr: 0.0010\n",
      "Epoch 38/70\n",
      "391/391 [==============================] - 15s 38ms/step - loss: 0.3703 - accuracy: 0.9681 - val_loss: 1.6223 - val_accuracy: 0.7291 - lr: 0.0010\n",
      "Epoch 39/70\n",
      "391/391 [==============================] - 14s 37ms/step - loss: 0.3712 - accuracy: 0.9687 - val_loss: 2.1052 - val_accuracy: 0.7035 - lr: 0.0010\n",
      "Epoch 40/70\n",
      "391/391 [==============================] - 14s 37ms/step - loss: 0.3633 - accuracy: 0.9706 - val_loss: 1.8633 - val_accuracy: 0.7005 - lr: 0.0010\n",
      "Epoch 41/70\n",
      "391/391 [==============================] - 15s 38ms/step - loss: 0.3608 - accuracy: 0.9718 - val_loss: 1.3338 - val_accuracy: 0.7616 - lr: 0.0010\n",
      "Epoch 42/70\n",
      "391/391 [==============================] - 15s 38ms/step - loss: 0.3605 - accuracy: 0.9707 - val_loss: 1.8598 - val_accuracy: 0.7045 - lr: 0.0010\n",
      "Epoch 43/70\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.3599 - accuracy: 0.9712 - val_loss: 1.2575 - val_accuracy: 0.7677 - lr: 0.0010\n",
      "Epoch 44/70\n",
      "391/391 [==============================] - 15s 38ms/step - loss: 0.3572 - accuracy: 0.9720 - val_loss: 1.9005 - val_accuracy: 0.7108 - lr: 0.0010\n",
      "Epoch 45/70\n",
      "391/391 [==============================] - 15s 38ms/step - loss: 0.3506 - accuracy: 0.9737 - val_loss: 1.4419 - val_accuracy: 0.7572 - lr: 0.0010\n",
      "Epoch 46/70\n",
      "391/391 [==============================] - 14s 37ms/step - loss: 0.2965 - accuracy: 0.9933 - val_loss: 0.9866 - val_accuracy: 0.8310 - lr: 1.0000e-04\n",
      "Epoch 47/70\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.2768 - accuracy: 0.9992 - val_loss: 0.9949 - val_accuracy: 0.8312 - lr: 1.0000e-04\n",
      "Epoch 48/70\n",
      "391/391 [==============================] - 15s 38ms/step - loss: 0.2701 - accuracy: 0.9999 - val_loss: 0.9995 - val_accuracy: 0.8323 - lr: 1.0000e-04\n",
      "Epoch 49/70\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.2642 - accuracy: 0.9999 - val_loss: 1.0191 - val_accuracy: 0.8342 - lr: 1.0000e-04\n",
      "Epoch 50/70\n",
      "391/391 [==============================] - 15s 38ms/step - loss: 0.2580 - accuracy: 1.0000 - val_loss: 1.0338 - val_accuracy: 0.8326 - lr: 1.0000e-04\n",
      "Epoch 51/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 14s 37ms/step - loss: 0.2515 - accuracy: 0.9999 - val_loss: 1.0419 - val_accuracy: 0.8326 - lr: 1.0000e-04\n",
      "Epoch 52/70\n",
      "391/391 [==============================] - 15s 38ms/step - loss: 0.2438 - accuracy: 1.0000 - val_loss: 1.0534 - val_accuracy: 0.8317 - lr: 1.0000e-04\n",
      "Epoch 53/70\n",
      "391/391 [==============================] - 15s 38ms/step - loss: 0.2354 - accuracy: 1.0000 - val_loss: 1.0555 - val_accuracy: 0.8322 - lr: 1.0000e-04\n",
      "Epoch 54/70\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.2258 - accuracy: 1.0000 - val_loss: 1.0483 - val_accuracy: 0.8306 - lr: 1.0000e-04\n",
      "Epoch 55/70\n",
      "391/391 [==============================] - 15s 37ms/step - loss: 0.2154 - accuracy: 1.0000 - val_loss: 1.0547 - val_accuracy: 0.8336 - lr: 1.0000e-04\n",
      "Epoch 56/70\n",
      "391/391 [==============================] - 15s 38ms/step - loss: 0.2037 - accuracy: 1.0000 - val_loss: 1.0596 - val_accuracy: 0.8314 - lr: 1.0000e-04\n",
      "Epoch 57/70\n",
      "391/391 [==============================] - 15s 37ms/step - loss: 0.1911 - accuracy: 1.0000 - val_loss: 1.0645 - val_accuracy: 0.8294 - lr: 1.0000e-04\n",
      "Epoch 58/70\n",
      "391/391 [==============================] - 15s 38ms/step - loss: 0.1779 - accuracy: 1.0000 - val_loss: 1.0809 - val_accuracy: 0.8254 - lr: 1.0000e-04\n",
      "Epoch 59/70\n",
      "391/391 [==============================] - 15s 38ms/step - loss: 0.1719 - accuracy: 0.9984 - val_loss: 1.2719 - val_accuracy: 0.8066 - lr: 1.0000e-04\n",
      "Epoch 60/70\n",
      "391/391 [==============================] - 15s 37ms/step - loss: 0.1643 - accuracy: 0.9995 - val_loss: 1.0950 - val_accuracy: 0.8232 - lr: 1.0000e-04\n",
      "Epoch 61/70\n",
      "391/391 [==============================] - 15s 38ms/step - loss: 0.1605 - accuracy: 0.9999 - val_loss: 1.0879 - val_accuracy: 0.8278 - lr: 1.0000e-05\n",
      "Epoch 62/70\n",
      "391/391 [==============================] - 14s 37ms/step - loss: 0.1595 - accuracy: 0.9999 - val_loss: 1.0833 - val_accuracy: 0.8292 - lr: 1.0000e-05\n",
      "Epoch 63/70\n",
      "391/391 [==============================] - 15s 37ms/step - loss: 0.1587 - accuracy: 1.0000 - val_loss: 1.0820 - val_accuracy: 0.8297 - lr: 1.0000e-05\n",
      "Epoch 64/70\n",
      "391/391 [==============================] - 15s 38ms/step - loss: 0.1580 - accuracy: 1.0000 - val_loss: 1.0843 - val_accuracy: 0.8308 - lr: 1.0000e-05\n",
      "Epoch 65/70\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.1571 - accuracy: 1.0000 - val_loss: 1.0833 - val_accuracy: 0.8299 - lr: 1.0000e-05\n",
      "Epoch 66/70\n",
      "391/391 [==============================] - 15s 38ms/step - loss: 0.1562 - accuracy: 1.0000 - val_loss: 1.0815 - val_accuracy: 0.8300 - lr: 1.0000e-05\n",
      "Epoch 67/70\n",
      "391/391 [==============================] - 15s 38ms/step - loss: 0.1550 - accuracy: 1.0000 - val_loss: 1.0804 - val_accuracy: 0.8295 - lr: 1.0000e-05\n",
      "Epoch 68/70\n",
      "391/391 [==============================] - 15s 38ms/step - loss: 0.1538 - accuracy: 1.0000 - val_loss: 1.0771 - val_accuracy: 0.8302 - lr: 1.0000e-05\n",
      "Epoch 69/70\n",
      "391/391 [==============================] - 15s 37ms/step - loss: 0.1524 - accuracy: 1.0000 - val_loss: 1.0835 - val_accuracy: 0.8307 - lr: 1.0000e-05\n",
      "Epoch 70/70\n",
      "391/391 [==============================] - 15s 38ms/step - loss: 0.1509 - accuracy: 1.0000 - val_loss: 1.0789 - val_accuracy: 0.8315 - lr: 1.0000e-05\n",
      "Epoch 1/70\n",
      "  2/390 [..............................] - ETA: 1:29 - loss: 5.0982 - accuracy: 0.0938WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.213501). Check your callbacks.\n",
      "391/390 [==============================] - 49s 125ms/step - loss: 1.9440 - accuracy: 0.4435 - val_loss: 2.4468 - val_accuracy: 0.3309 - lr: 0.0010\n",
      "Epoch 2/70\n",
      "391/390 [==============================] - 18s 45ms/step - loss: 1.4813 - accuracy: 0.6057 - val_loss: 3.8212 - val_accuracy: 0.3450 - lr: 0.0010\n",
      "Epoch 3/70\n",
      "391/390 [==============================] - 18s 46ms/step - loss: 1.2500 - accuracy: 0.6846 - val_loss: 1.3667 - val_accuracy: 0.6615 - lr: 0.0010\n",
      "Epoch 4/70\n",
      "391/390 [==============================] - 18s 46ms/step - loss: 1.0981 - accuracy: 0.7335 - val_loss: 1.3414 - val_accuracy: 0.6709 - lr: 0.0010\n",
      "Epoch 5/70\n",
      "391/390 [==============================] - 18s 47ms/step - loss: 0.9862 - accuracy: 0.7660 - val_loss: 1.4457 - val_accuracy: 0.6370 - lr: 0.0010\n",
      "Epoch 6/70\n",
      "391/390 [==============================] - 18s 46ms/step - loss: 0.9129 - accuracy: 0.7876 - val_loss: 1.1850 - val_accuracy: 0.7044 - lr: 0.0010\n",
      "Epoch 7/70\n",
      "391/390 [==============================] - 18s 47ms/step - loss: 0.8494 - accuracy: 0.8061 - val_loss: 1.0807 - val_accuracy: 0.7313 - lr: 0.0010\n",
      "Epoch 8/70\n",
      "391/390 [==============================] - 18s 45ms/step - loss: 0.8076 - accuracy: 0.8165 - val_loss: 1.0515 - val_accuracy: 0.7353 - lr: 0.0010\n",
      "Epoch 9/70\n",
      "391/390 [==============================] - 18s 45ms/step - loss: 0.7647 - accuracy: 0.8289 - val_loss: 1.0741 - val_accuracy: 0.7289 - lr: 0.0010\n",
      "Epoch 10/70\n",
      "391/390 [==============================] - 18s 46ms/step - loss: 0.7287 - accuracy: 0.8382 - val_loss: 0.9910 - val_accuracy: 0.7693 - lr: 0.0010\n",
      "Epoch 11/70\n",
      "391/390 [==============================] - 18s 46ms/step - loss: 0.7070 - accuracy: 0.8415 - val_loss: 0.9112 - val_accuracy: 0.7791 - lr: 0.0010\n",
      "Epoch 12/70\n",
      "391/390 [==============================] - 18s 46ms/step - loss: 0.6797 - accuracy: 0.8505 - val_loss: 1.0181 - val_accuracy: 0.7498 - lr: 0.0010\n",
      "Epoch 13/70\n",
      "391/390 [==============================] - 18s 46ms/step - loss: 0.6574 - accuracy: 0.8564 - val_loss: 1.1186 - val_accuracy: 0.7381 - lr: 0.0010\n",
      "Epoch 14/70\n",
      "391/390 [==============================] - 18s 46ms/step - loss: 0.6370 - accuracy: 0.8625 - val_loss: 0.9864 - val_accuracy: 0.7666 - lr: 0.0010\n",
      "Epoch 15/70\n",
      "391/390 [==============================] - 18s 46ms/step - loss: 0.6162 - accuracy: 0.8681 - val_loss: 0.9273 - val_accuracy: 0.7754 - lr: 0.0010\n",
      "Epoch 16/70\n",
      "391/390 [==============================] - 18s 45ms/step - loss: 0.6091 - accuracy: 0.8702 - val_loss: 1.0188 - val_accuracy: 0.7503 - lr: 0.0010\n",
      "Epoch 17/70\n",
      "391/390 [==============================] - 18s 47ms/step - loss: 0.5857 - accuracy: 0.8770 - val_loss: 0.7879 - val_accuracy: 0.8142 - lr: 0.0010\n",
      "Epoch 18/70\n",
      "391/390 [==============================] - 18s 46ms/step - loss: 0.5765 - accuracy: 0.8787 - val_loss: 0.8619 - val_accuracy: 0.7983 - lr: 0.0010\n",
      "Epoch 19/70\n",
      "391/390 [==============================] - 18s 45ms/step - loss: 0.5667 - accuracy: 0.8820 - val_loss: 0.8706 - val_accuracy: 0.8013 - lr: 0.0010\n",
      "Epoch 20/70\n",
      "391/390 [==============================] - 18s 46ms/step - loss: 0.5558 - accuracy: 0.8862 - val_loss: 0.8671 - val_accuracy: 0.7920 - lr: 0.0010\n",
      "Epoch 21/70\n",
      "391/390 [==============================] - 18s 45ms/step - loss: 0.5412 - accuracy: 0.8909 - val_loss: 0.7830 - val_accuracy: 0.8174 - lr: 0.0010\n",
      "Epoch 22/70\n",
      "391/390 [==============================] - 18s 45ms/step - loss: 0.5336 - accuracy: 0.8925 - val_loss: 0.8922 - val_accuracy: 0.7971 - lr: 0.0010\n",
      "Epoch 23/70\n",
      "391/390 [==============================] - 18s 46ms/step - loss: 0.5207 - accuracy: 0.8969 - val_loss: 0.8934 - val_accuracy: 0.8094 - lr: 0.0010\n",
      "Epoch 24/70\n",
      "391/390 [==============================] - 18s 46ms/step - loss: 0.5169 - accuracy: 0.8979 - val_loss: 0.8641 - val_accuracy: 0.7994 - lr: 0.0010\n",
      "Epoch 25/70\n",
      "391/390 [==============================] - 18s 45ms/step - loss: 0.5103 - accuracy: 0.8993 - val_loss: 0.9075 - val_accuracy: 0.7974 - lr: 0.0010\n",
      "Epoch 26/70\n",
      "391/390 [==============================] - 18s 46ms/step - loss: 0.5051 - accuracy: 0.9028 - val_loss: 0.9473 - val_accuracy: 0.7855 - lr: 0.0010\n",
      "Epoch 27/70\n",
      "391/390 [==============================] - 18s 46ms/step - loss: 0.4943 - accuracy: 0.9061 - val_loss: 0.8813 - val_accuracy: 0.8004 - lr: 0.0010\n",
      "Epoch 28/70\n",
      "391/390 [==============================] - 18s 45ms/step - loss: 0.4853 - accuracy: 0.9076 - val_loss: 0.7193 - val_accuracy: 0.8376 - lr: 0.0010\n",
      "Epoch 29/70\n",
      "391/390 [==============================] - 18s 46ms/step - loss: 0.4829 - accuracy: 0.9101 - val_loss: 0.7768 - val_accuracy: 0.8226 - lr: 0.0010\n",
      "Epoch 30/70\n",
      "391/390 [==============================] - 18s 45ms/step - loss: 0.4783 - accuracy: 0.9104 - val_loss: 0.8322 - val_accuracy: 0.8146 - lr: 0.0010\n",
      "Epoch 31/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/390 [==============================] - 18s 46ms/step - loss: 0.4772 - accuracy: 0.9102 - val_loss: 0.7067 - val_accuracy: 0.8490 - lr: 0.0010\n",
      "Epoch 32/70\n",
      "391/390 [==============================] - 18s 47ms/step - loss: 0.4678 - accuracy: 0.9141 - val_loss: 0.7899 - val_accuracy: 0.8235 - lr: 0.0010\n",
      "Epoch 33/70\n",
      "391/390 [==============================] - 18s 46ms/step - loss: 0.4683 - accuracy: 0.9141 - val_loss: 0.9306 - val_accuracy: 0.7873 - lr: 0.0010\n",
      "Epoch 34/70\n",
      "391/390 [==============================] - 18s 47ms/step - loss: 0.4646 - accuracy: 0.9161 - val_loss: 0.6567 - val_accuracy: 0.8584 - lr: 0.0010\n",
      "Epoch 35/70\n",
      "391/390 [==============================] - 18s 45ms/step - loss: 0.4599 - accuracy: 0.9173 - val_loss: 0.8775 - val_accuracy: 0.8118 - lr: 0.0010\n",
      "Epoch 36/70\n",
      "391/390 [==============================] - 19s 47ms/step - loss: 0.4577 - accuracy: 0.9185 - val_loss: 1.2522 - val_accuracy: 0.7282 - lr: 0.0010\n",
      "Epoch 37/70\n",
      "391/390 [==============================] - 18s 46ms/step - loss: 0.4527 - accuracy: 0.9196 - val_loss: 0.9642 - val_accuracy: 0.7911 - lr: 0.0010\n",
      "Epoch 38/70\n",
      "391/390 [==============================] - 18s 46ms/step - loss: 0.4463 - accuracy: 0.9223 - val_loss: 1.0313 - val_accuracy: 0.7906 - lr: 0.0010\n",
      "Epoch 39/70\n",
      "391/390 [==============================] - 18s 47ms/step - loss: 0.4543 - accuracy: 0.9195 - val_loss: 0.7971 - val_accuracy: 0.8346 - lr: 0.0010\n",
      "Epoch 40/70\n",
      "391/390 [==============================] - 18s 45ms/step - loss: 0.4413 - accuracy: 0.9243 - val_loss: 0.9592 - val_accuracy: 0.7976 - lr: 0.0010\n",
      "Epoch 41/70\n",
      "391/390 [==============================] - 18s 45ms/step - loss: 0.4366 - accuracy: 0.9252 - val_loss: 0.7986 - val_accuracy: 0.8250 - lr: 0.0010\n",
      "Epoch 42/70\n",
      "391/390 [==============================] - 18s 45ms/step - loss: 0.4424 - accuracy: 0.9240 - val_loss: 0.8165 - val_accuracy: 0.8255 - lr: 0.0010\n",
      "Epoch 43/70\n",
      "391/390 [==============================] - 18s 45ms/step - loss: 0.4371 - accuracy: 0.9255 - val_loss: 0.8514 - val_accuracy: 0.8209 - lr: 0.0010\n",
      "Epoch 44/70\n",
      "391/390 [==============================] - 18s 47ms/step - loss: 0.4347 - accuracy: 0.9259 - val_loss: 0.8398 - val_accuracy: 0.8208 - lr: 0.0010\n",
      "Epoch 45/70\n",
      "391/390 [==============================] - 18s 46ms/step - loss: 0.4332 - accuracy: 0.9267 - val_loss: 0.7688 - val_accuracy: 0.8277 - lr: 0.0010\n",
      "Epoch 46/70\n",
      "391/390 [==============================] - 18s 45ms/step - loss: 0.3534 - accuracy: 0.9568 - val_loss: 0.4955 - val_accuracy: 0.9097 - lr: 1.0000e-04\n",
      "Epoch 47/70\n",
      "391/390 [==============================] - 18s 46ms/step - loss: 0.3184 - accuracy: 0.9682 - val_loss: 0.4984 - val_accuracy: 0.9116 - lr: 1.0000e-04\n",
      "Epoch 48/70\n",
      "391/390 [==============================] - 18s 45ms/step - loss: 0.3046 - accuracy: 0.9715 - val_loss: 0.5021 - val_accuracy: 0.9101 - lr: 1.0000e-04\n",
      "Epoch 49/70\n",
      "391/390 [==============================] - 18s 45ms/step - loss: 0.2922 - accuracy: 0.9747 - val_loss: 0.5064 - val_accuracy: 0.9113 - lr: 1.0000e-04\n",
      "Epoch 50/70\n",
      "391/390 [==============================] - 18s 46ms/step - loss: 0.2804 - accuracy: 0.9773 - val_loss: 0.5136 - val_accuracy: 0.9094 - lr: 1.0000e-04\n",
      "Epoch 51/70\n",
      "391/390 [==============================] - 17s 45ms/step - loss: 0.2724 - accuracy: 0.9794 - val_loss: 0.5029 - val_accuracy: 0.9136 - lr: 1.0000e-04\n",
      "Epoch 52/70\n",
      "391/390 [==============================] - 18s 46ms/step - loss: 0.2653 - accuracy: 0.9803 - val_loss: 0.5120 - val_accuracy: 0.9121 - lr: 1.0000e-04\n",
      "Epoch 53/70\n",
      "391/390 [==============================] - 18s 46ms/step - loss: 0.2567 - accuracy: 0.9824 - val_loss: 0.5088 - val_accuracy: 0.9130 - lr: 1.0000e-04\n",
      "Epoch 54/70\n",
      "391/390 [==============================] - 18s 46ms/step - loss: 0.2511 - accuracy: 0.9833 - val_loss: 0.5255 - val_accuracy: 0.9133 - lr: 1.0000e-04\n",
      "Epoch 55/70\n",
      "391/390 [==============================] - 18s 45ms/step - loss: 0.2471 - accuracy: 0.9837 - val_loss: 0.5175 - val_accuracy: 0.9114 - lr: 1.0000e-04\n",
      "Epoch 56/70\n",
      "391/390 [==============================] - 18s 46ms/step - loss: 0.2413 - accuracy: 0.9850 - val_loss: 0.5221 - val_accuracy: 0.9113 - lr: 1.0000e-04\n",
      "Epoch 57/70\n",
      "391/390 [==============================] - 18s 45ms/step - loss: 0.2349 - accuracy: 0.9862 - val_loss: 0.5299 - val_accuracy: 0.9112 - lr: 1.0000e-04\n",
      "Epoch 58/70\n",
      "391/390 [==============================] - 18s 47ms/step - loss: 0.2317 - accuracy: 0.9865 - val_loss: 0.5332 - val_accuracy: 0.9098 - lr: 1.0000e-04\n",
      "Epoch 59/70\n",
      "391/390 [==============================] - 18s 45ms/step - loss: 0.2253 - accuracy: 0.9882 - val_loss: 0.5162 - val_accuracy: 0.9122 - lr: 1.0000e-04\n",
      "Epoch 60/70\n",
      "391/390 [==============================] - 18s 46ms/step - loss: 0.2233 - accuracy: 0.9879 - val_loss: 0.5395 - val_accuracy: 0.9125 - lr: 1.0000e-04\n",
      "Epoch 61/70\n",
      "391/390 [==============================] - 18s 46ms/step - loss: 0.2163 - accuracy: 0.9900 - val_loss: 0.5116 - val_accuracy: 0.9155 - lr: 1.0000e-05\n",
      "Epoch 62/70\n",
      "391/390 [==============================] - 18s 46ms/step - loss: 0.2118 - accuracy: 0.9917 - val_loss: 0.5105 - val_accuracy: 0.9155 - lr: 1.0000e-05\n",
      "Epoch 63/70\n",
      "391/390 [==============================] - 18s 45ms/step - loss: 0.2113 - accuracy: 0.9923 - val_loss: 0.5106 - val_accuracy: 0.9165 - lr: 1.0000e-05\n",
      "Epoch 64/70\n",
      "391/390 [==============================] - 18s 46ms/step - loss: 0.2090 - accuracy: 0.9927 - val_loss: 0.5107 - val_accuracy: 0.9173 - lr: 1.0000e-05\n",
      "Epoch 65/70\n",
      "391/390 [==============================] - 18s 46ms/step - loss: 0.2078 - accuracy: 0.9935 - val_loss: 0.5129 - val_accuracy: 0.9169 - lr: 1.0000e-05\n",
      "Epoch 66/70\n",
      "391/390 [==============================] - 18s 46ms/step - loss: 0.2082 - accuracy: 0.9929 - val_loss: 0.5131 - val_accuracy: 0.9166 - lr: 1.0000e-05\n",
      "Epoch 67/70\n",
      "391/390 [==============================] - 18s 45ms/step - loss: 0.2079 - accuracy: 0.9926 - val_loss: 0.5143 - val_accuracy: 0.9175 - lr: 1.0000e-05\n",
      "Epoch 68/70\n",
      "391/390 [==============================] - 18s 46ms/step - loss: 0.2059 - accuracy: 0.9934 - val_loss: 0.5148 - val_accuracy: 0.9169 - lr: 1.0000e-05\n",
      "Epoch 69/70\n",
      "391/390 [==============================] - 18s 46ms/step - loss: 0.2062 - accuracy: 0.9934 - val_loss: 0.5166 - val_accuracy: 0.9177 - lr: 1.0000e-05\n",
      "Epoch 70/70\n",
      "391/390 [==============================] - 17s 45ms/step - loss: 0.2048 - accuracy: 0.9935 - val_loss: 0.5172 - val_accuracy: 0.9175 - lr: 1.0000e-05\n"
     ]
    }
   ],
   "source": [
    "def learning_rate_schedule(epoch):\n",
    "    \"\"\"Learning rate is scheduled to be reduced after 45 and 60 epochs.\n",
    "    This function is automatically every epoch as part of callbacks\n",
    "    during training.\n",
    "    \"\"\"\n",
    "    if epoch < 45:\n",
    "        return 1e-3\n",
    "    if epoch < 60:\n",
    "        return 1e-4\n",
    "    return 1e-5\n",
    "\n",
    "def train_with_lr_decay(model):\n",
    "    model.compile(\n",
    "        loss='sparse_categorical_crossentropy', metrics=['accuracy'],\n",
    "        optimizer=optimizers.Adam(lr=1e-3))\n",
    "\n",
    "    # Callback for learning rate adjustment (see below)\n",
    "    lr_scheduler = callbacks.LearningRateScheduler(learning_rate_schedule)\n",
    "\n",
    "    # TensorBoard callback\n",
    "    timestamp = datetime.datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "    logdir = os.path.join(log_root, f'{model.name}_{timestamp}')\n",
    "    tensorboard_callback = callbacks.TensorBoard(logdir, histogram_freq=1)\n",
    "    \n",
    "    # Fit the model on the batches generated by datagen.flow()\n",
    "    model.fit(\n",
    "        x_train, y_train, batch_size=128,\n",
    "        validation_data=(x_test, y_test), epochs=70, verbose=1, \n",
    "        callbacks=[lr_scheduler, tensorboard_callback])\n",
    "    \n",
    "def train_with_lr_decay_and_augmentation(model):\n",
    "    model.compile(\n",
    "        loss='categorical_crossentropy', metrics=['accuracy'],\n",
    "        optimizer=optimizers.Adam(lr=1e-3))\n",
    "\n",
    "    # Callback for learning rate adjustment (see below)\n",
    "    lr_scheduler = callbacks.LearningRateScheduler(learning_rate_schedule)\n",
    "\n",
    "    # TensorBoard callback\n",
    "    timestamp = datetime.datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "    logdir = os.path.join(log_root, f'{model.name}_augmented_{timestamp}')\n",
    "    tensorboard_callback = callbacks.TensorBoard(logdir, histogram_freq=1)\n",
    "\n",
    "    # Data augmentation: flip and shift horizontally/vertically by max 4 pixels\n",
    "    # datagen = kerasimage.ImageDataGenerator(...)\n",
    "    ### BEGIN SOLUTION\n",
    "    datagen = kerasimage.ImageDataGenerator(\n",
    "        width_shift_range=4, height_shift_range=4,\n",
    "        horizontal_flip=True, fill_mode='constant')\n",
    "    ### END SOLUTION\n",
    "    \n",
    "    # Note: model.fit with generator input seems to only work when the\n",
    "    # y targets are provided as one-hot vectors\n",
    "    y_train_onehot = tf.keras.utils.to_categorical(y_train, 10)\n",
    "    y_test_onehot = tf.keras.utils.to_categorical(y_test, 10)\n",
    "    \n",
    "    # Fit the model on the batches generated by datagen.flow() using model.fit()\n",
    "    ### BEGIN SOLUTION\n",
    "    model.fit(\n",
    "        datagen.flow(x_train, y_train_onehot, batch_size=128),\n",
    "        validation_data=(x_test, y_test_onehot),\n",
    "        steps_per_epoch=len(x_train) / 128, epochs=70, verbose=1, \n",
    "        callbacks=[lr_scheduler, tensorboard_callback])\n",
    "    ### END SOLUTION\n",
    "\n",
    "resnet56 = resnet(56)\n",
    "train_with_lr_decay(resnet56)\n",
    "resnet56 = resnet(56)\n",
    "train_with_lr_decay_and_augmentation(resnet56)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9cde4357375a57bf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Does the augmentation improve the final performance? What do you observe on the training and validation curves compared to no augmentation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-b96d266d8e522852",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "Yes, the accuracy on the validation set gets better. The training accuracy grows slower due to the augmentation because the model can not overfit as easy anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
